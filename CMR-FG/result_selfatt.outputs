Namespace(RNN_dropout=0.0, WORKERS=0, audio_model='Davenet', batch_size=64, cfg_file='Confg/flower_train.yml', data_path='/tudelft.net/staff-bulk/ewi/insy/SpeechLab/TianTian/data/flowers/Oxford102', exp_dir='', gpu_id=0, image_model='VGG16', img_size=256, lr=0.001, lr_decay=100, manualSeed=200, margin=1.0, momentum=0.9, n_epochs=120, n_heads=5, n_print_steps=2, optim='adam', pretrained_image_model=False, result_file=None, resume=True, rnn_type='LSTM', save_root='outputs/01_Baseline/Oxford102/full', simtype='MISA', smooth_gamm3=5.0, start_epoch=0, tasks='extraction', topk=3, weight_decay=0.001)
Load filenames from: /tudelft.net/staff-bulk/ewi/insy/SpeechLab/TianTian/data/flowers/Oxford102/train/filenames.pickle (7034)
Load filenames from: /tudelft.net/staff-bulk/ewi/insy/SpeechLab/TianTian/data/flowers/Oxford102/test/filenames.pickle (1155)
Load filenames from: /tudelft.net/staff-bulk/ewi/insy/SpeechLab/TianTian/data/flowers/Oxford102/test/filenames.pickle (1155)
current #steps=0, #epochs=0
start training...
iteration = 0 | loss = 8.302658 
iteration = 5 | loss = 8.402662 
iteration = 10 | loss = 8.381678 
iteration = 15 | loss = 8.376151 
iteration = 20 | loss = 8.364123 
iteration = 25 | loss = 8.364224 
iteration = 30 | loss = 8.347591 
iteration = 35 | loss = 8.349319 
iteration = 40 | loss = 8.308952 
iteration = 45 | loss = 8.328737 
iteration = 50 | loss = 8.322688 
iteration = 55 | loss = 8.301932 
iteration = 60 | loss = 8.285465 
iteration = 65 | loss = 8.299281 
iteration = 70 | loss = 8.223137 
iteration = 75 | loss = 8.253300 
iteration = 80 | loss = 8.201638 
iteration = 85 | loss = 8.245455 
iteration = 90 | loss = 8.295464 
iteration = 95 | loss = 8.167261 
iteration = 100 | loss = 8.277976 
iteration = 105 | loss = 8.153576 
iteration = 0 | loss = 8.185814 
iteration = 5 | loss = 8.058851 
iteration = 10 | loss = 7.951753 
iteration = 15 | loss = 7.904937 
iteration = 20 | loss = 7.827211 
iteration = 25 | loss = 7.953176 
iteration = 30 | loss = 7.986345 
iteration = 35 | loss = 8.079778 
iteration = 40 | loss = 7.901471 
iteration = 45 | loss = 7.764291 
iteration = 50 | loss = 7.795506 
iteration = 55 | loss = 7.971516 
iteration = 60 | loss = 7.564423 
iteration = 65 | loss = 7.583039 
iteration = 70 | loss = 7.752154 
iteration = 75 | loss = 8.009430 
iteration = 80 | loss = 7.459141 
iteration = 85 | loss = 7.755491 
iteration = 90 | loss = 7.876499 
iteration = 95 | loss = 7.719477 
iteration = 100 | loss = 7.237010 
iteration = 105 | loss = 7.058189 
iteration = 0 | loss = 7.105854 
iteration = 5 | loss = 7.239874 
iteration = 10 | loss = 7.354944 
iteration = 15 | loss = 7.100239 
iteration = 20 | loss = 6.912847 
iteration = 25 | loss = 7.082493 
iteration = 30 | loss = 7.326729 
iteration = 35 | loss = 7.167245 
iteration = 40 | loss = 7.019025 
iteration = 45 | loss = 6.890769 
iteration = 50 | loss = 6.762614 
iteration = 55 | loss = 7.290067 
iteration = 60 | loss = 7.025029 
iteration = 65 | loss = 6.703012 
iteration = 70 | loss = 7.001595 
iteration = 75 | loss = 6.884514 
iteration = 80 | loss = 6.641801 
iteration = 85 | loss = 6.933760 
iteration = 90 | loss = 6.909086 
iteration = 95 | loss = 6.557276 
iteration = 100 | loss = 6.709456 
iteration = 105 | loss = 6.169638 
iteration = 0 | loss = 6.292611 
iteration = 5 | loss = 6.624435 
iteration = 10 | loss = 6.386961 
iteration = 15 | loss = 6.478832 
iteration = 20 | loss = 5.923523 
iteration = 25 | loss = 6.562800 
iteration = 30 | loss = 6.435444 
iteration = 35 | loss = 6.390245 
iteration = 40 | loss = 6.163527 
iteration = 45 | loss = 6.312574 
iteration = 50 | loss = 6.439404 
iteration = 55 | loss = 6.209383 
iteration = 60 | loss = 6.444403 
iteration = 65 | loss = 6.252273 
iteration = 70 | loss = 6.223535 
iteration = 75 | loss = 5.941388 
iteration = 80 | loss = 5.856944 
iteration = 85 | loss = 5.820444 
iteration = 90 | loss = 5.925217 
iteration = 95 | loss = 6.159710 
iteration = 100 | loss = 6.470206 
iteration = 105 | loss = 5.776049 
iteration = 0 | loss = 6.044474 
iteration = 5 | loss = 5.888012 
iteration = 10 | loss = 5.829448 
iteration = 15 | loss = 6.045856 
iteration = 20 | loss = 6.019454 
iteration = 25 | loss = 5.782076 
iteration = 30 | loss = 6.050676 
iteration = 35 | loss = 5.581702 
iteration = 40 | loss = 5.673611 
iteration = 45 | loss = 5.868529 
iteration = 50 | loss = 5.585860 
iteration = 55 | loss = 5.554438 
iteration = 60 | loss = 5.729646 
iteration = 65 | loss = 5.659202 
iteration = 70 | loss = 5.783610 
iteration = 75 | loss = 5.879317 
iteration = 80 | loss = 5.606433 
iteration = 85 | loss = 6.189951 
iteration = 90 | loss = 5.697179 
iteration = 95 | loss = 5.916073 
iteration = 100 | loss = 5.622759 
iteration = 105 | loss = 5.724512 
 Epoch: [5] Loss: 5.5666  R1_I2A: 0.3342 R1_A2I: 0.2242 
                 
iteration = 0 | loss = 5.615413 
iteration = 5 | loss = 5.520839 
iteration = 10 | loss = 5.886611 
iteration = 15 | loss = 5.705873 
iteration = 20 | loss = 5.919692 
iteration = 25 | loss = 5.464952 
iteration = 30 | loss = 5.569786 
iteration = 35 | loss = 5.543448 
iteration = 40 | loss = 5.459945 
iteration = 45 | loss = 5.794620 
iteration = 50 | loss = 6.122986 
iteration = 55 | loss = 5.923356 
iteration = 60 | loss = 5.970823 
iteration = 65 | loss = 5.681728 
iteration = 70 | loss = 5.580976 
iteration = 75 | loss = 5.473184 
iteration = 80 | loss = 5.566799 
iteration = 85 | loss = 5.675471 
iteration = 90 | loss = 5.776291 
iteration = 95 | loss = 5.591746 
iteration = 100 | loss = 5.586098 
iteration = 105 | loss = 5.437165 
iteration = 0 | loss = 5.443723 
iteration = 5 | loss = 5.631961 
iteration = 10 | loss = 5.839488 
iteration = 15 | loss = 5.572540 
iteration = 20 | loss = 5.457264 
iteration = 25 | loss = 5.608978 
iteration = 30 | loss = 5.447480 
iteration = 35 | loss = 5.546341 
iteration = 40 | loss = 5.509134 
iteration = 45 | loss = 5.728668 
iteration = 50 | loss = 5.358128 
iteration = 55 | loss = 5.805572 
iteration = 60 | loss = 5.322404 
iteration = 65 | loss = 5.357815 
iteration = 70 | loss = 5.885973 
iteration = 75 | loss = 5.330113 
iteration = 80 | loss = 5.643399 
iteration = 85 | loss = 5.457685 
iteration = 90 | loss = 5.892879 
iteration = 95 | loss = 5.337255 
iteration = 100 | loss = 5.351763 
iteration = 105 | loss = 5.761800 
iteration = 0 | loss = 5.545488 
iteration = 5 | loss = 5.397638 
iteration = 10 | loss = 5.620150 
iteration = 15 | loss = 5.227807 
iteration = 20 | loss = 5.059551 
iteration = 25 | loss = 5.606814 
iteration = 30 | loss = 5.305934 
iteration = 35 | loss = 5.271983 
iteration = 40 | loss = 5.355262 
iteration = 45 | loss = 5.625879 
iteration = 50 | loss = 5.809149 
iteration = 55 | loss = 5.854044 
iteration = 60 | loss = 5.515734 
iteration = 65 | loss = 5.524439 
iteration = 70 | loss = 5.521334 
iteration = 75 | loss = 5.133232 
iteration = 80 | loss = 5.696998 
iteration = 85 | loss = 5.730256 
iteration = 90 | loss = 5.246963 
iteration = 95 | loss = 5.271578 
iteration = 100 | loss = 5.356763 
iteration = 105 | loss = 5.437609 
iteration = 0 | loss = 5.111738 
iteration = 5 | loss = 5.945230 
iteration = 10 | loss = 5.548282 
iteration = 15 | loss = 5.701091 
iteration = 20 | loss = 5.995413 
iteration = 25 | loss = 5.413990 
iteration = 30 | loss = 5.475507 
iteration = 35 | loss = 5.815896 
iteration = 40 | loss = 5.617695 
iteration = 45 | loss = 4.825627 
iteration = 50 | loss = 5.311302 
iteration = 55 | loss = 5.042085 
iteration = 60 | loss = 5.658072 
iteration = 65 | loss = 5.544898 
iteration = 70 | loss = 5.349638 
iteration = 75 | loss = 5.314676 
iteration = 80 | loss = 5.243511 
iteration = 85 | loss = 5.448718 
iteration = 90 | loss = 5.180418 
iteration = 95 | loss = 5.160223 
iteration = 100 | loss = 5.566084 
iteration = 105 | loss = 5.530996 
iteration = 0 | loss = 5.321388 
iteration = 5 | loss = 5.383950 
iteration = 10 | loss = 5.538570 
iteration = 15 | loss = 5.058514 
iteration = 20 | loss = 5.553034 
iteration = 25 | loss = 5.086836 
iteration = 30 | loss = 5.184202 
iteration = 35 | loss = 5.805092 
iteration = 40 | loss = 4.816761 
iteration = 45 | loss = 5.189650 
iteration = 50 | loss = 5.184790 
iteration = 55 | loss = 5.176617 
iteration = 60 | loss = 5.057373 
iteration = 65 | loss = 5.453581 
iteration = 70 | loss = 5.319942 
iteration = 75 | loss = 5.033461 
iteration = 80 | loss = 5.225489 
iteration = 85 | loss = 5.276933 
iteration = 90 | loss = 5.510164 
iteration = 95 | loss = 5.409922 
iteration = 100 | loss = 5.001863 
iteration = 105 | loss = 5.396214 
 Epoch: [10] Loss: 5.4233  R1_I2A: 0.3602 R1_A2I: 0.2918 
                 
iteration = 0 | loss = 5.772354 
iteration = 5 | loss = 5.686967 
iteration = 10 | loss = 5.279423 
iteration = 15 | loss = 5.486863 
iteration = 20 | loss = 5.357096 
iteration = 25 | loss = 5.725063 
iteration = 30 | loss = 5.511780 
iteration = 35 | loss = 4.996458 
iteration = 40 | loss = 5.337146 
iteration = 45 | loss = 5.298742 
iteration = 50 | loss = 4.846413 
iteration = 55 | loss = 5.437044 
iteration = 60 | loss = 4.983281 
iteration = 65 | loss = 5.232678 
iteration = 70 | loss = 5.269620 
iteration = 75 | loss = 5.388803 
iteration = 80 | loss = 5.273858 
iteration = 85 | loss = 5.364967 
iteration = 90 | loss = 5.078216 
iteration = 95 | loss = 5.205829 
iteration = 100 | loss = 5.293605 
iteration = 105 | loss = 5.376148 
iteration = 0 | loss = 5.190091 
iteration = 5 | loss = 5.304716 
iteration = 10 | loss = 5.443014 
iteration = 15 | loss = 5.147446 
iteration = 20 | loss = 5.264559 
iteration = 25 | loss = 5.298828 
iteration = 30 | loss = 5.368727 
iteration = 35 | loss = 5.099747 
iteration = 40 | loss = 5.191930 
iteration = 45 | loss = 5.154167 
iteration = 50 | loss = 4.949455 
iteration = 55 | loss = 5.304989 
iteration = 60 | loss = 5.423249 
iteration = 65 | loss = 5.195677 
iteration = 70 | loss = 5.632829 
iteration = 75 | loss = 4.885872 
iteration = 80 | loss = 5.256573 
iteration = 85 | loss = 5.083168 
iteration = 90 | loss = 4.915229 
iteration = 95 | loss = 5.250215 
iteration = 100 | loss = 5.336999 
iteration = 105 | loss = 5.366614 
iteration = 0 | loss = 5.029728 
iteration = 5 | loss = 5.207695 
iteration = 10 | loss = 5.421472 
iteration = 15 | loss = 5.649820 
iteration = 20 | loss = 4.935424 
iteration = 25 | loss = 5.099739 
iteration = 30 | loss = 5.177451 
iteration = 35 | loss = 5.487420 
iteration = 40 | loss = 5.082538 
iteration = 45 | loss = 5.109818 
iteration = 50 | loss = 5.673043 
iteration = 55 | loss = 5.350531 
iteration = 60 | loss = 5.439384 
iteration = 65 | loss = 5.461782 
iteration = 70 | loss = 5.408566 
iteration = 75 | loss = 5.119152 
iteration = 80 | loss = 5.274972 
iteration = 85 | loss = 5.326510 
iteration = 90 | loss = 5.300254 
iteration = 95 | loss = 4.772310 
iteration = 100 | loss = 5.167525 
iteration = 105 | loss = 5.120001 
iteration = 0 | loss = 4.936305 
iteration = 5 | loss = 5.748298 
iteration = 10 | loss = 4.949979 
iteration = 15 | loss = 5.353628 
iteration = 20 | loss = 5.222843 
iteration = 25 | loss = 4.658137 
iteration = 30 | loss = 5.203340 
iteration = 35 | loss = 5.433604 
iteration = 40 | loss = 5.257422 
iteration = 45 | loss = 5.298097 
iteration = 50 | loss = 5.123925 
iteration = 55 | loss = 5.020596 
iteration = 60 | loss = 4.989264 
iteration = 65 | loss = 4.968091 
iteration = 70 | loss = 5.542004 
iteration = 75 | loss = 5.900742 
iteration = 80 | loss = 5.212356 
iteration = 85 | loss = 4.996903 
iteration = 90 | loss = 5.275329 
iteration = 95 | loss = 5.401237 
iteration = 100 | loss = 5.223985 
iteration = 105 | loss = 5.050551 
iteration = 0 | loss = 5.043846 
iteration = 5 | loss = 5.124444 
iteration = 10 | loss = 5.011645 
iteration = 15 | loss = 4.903565 
iteration = 20 | loss = 5.123605 
iteration = 25 | loss = 5.320500 
iteration = 30 | loss = 5.085129 
iteration = 35 | loss = 4.868948 
iteration = 40 | loss = 5.525028 
iteration = 45 | loss = 5.617895 
iteration = 50 | loss = 5.390223 
iteration = 55 | loss = 5.366644 
iteration = 60 | loss = 4.846868 
iteration = 65 | loss = 5.128878 
iteration = 70 | loss = 5.279645 
iteration = 75 | loss = 5.174681 
iteration = 80 | loss = 5.431772 
iteration = 85 | loss = 5.169518 
iteration = 90 | loss = 5.033772 
iteration = 95 | loss = 4.762177 
iteration = 100 | loss = 4.794202 
iteration = 105 | loss = 5.261179 
 Epoch: [15] Loss: 5.0206  R1_I2A: 0.3991 R1_A2I: 0.3160 
                 
iteration = 0 | loss = 4.880547 
iteration = 5 | loss = 5.008306 
iteration = 10 | loss = 5.276142 
iteration = 15 | loss = 5.137624 
iteration = 20 | loss = 5.301348 
iteration = 25 | loss = 4.708460 
iteration = 30 | loss = 4.806082 
iteration = 35 | loss = 4.760853 
iteration = 40 | loss = 5.347103 
iteration = 45 | loss = 4.900991 
iteration = 50 | loss = 5.126703 
iteration = 55 | loss = 5.045874 
iteration = 60 | loss = 5.133924 
iteration = 65 | loss = 5.459912 
iteration = 70 | loss = 5.440040 
iteration = 75 | loss = 5.229105 
iteration = 80 | loss = 5.273266 
iteration = 85 | loss = 5.461561 
iteration = 90 | loss = 5.013420 
iteration = 95 | loss = 4.693278 
iteration = 100 | loss = 5.018981 
iteration = 105 | loss = 5.146295 
iteration = 0 | loss = 4.956454 
iteration = 5 | loss = 4.995650 
iteration = 10 | loss = 5.143906 
iteration = 15 | loss = 5.176184 
iteration = 20 | loss = 5.132477 
iteration = 25 | loss = 5.010990 
iteration = 30 | loss = 5.048182 
iteration = 35 | loss = 5.309505 
iteration = 40 | loss = 5.097246 
iteration = 45 | loss = 4.790156 
iteration = 50 | loss = 5.229403 
iteration = 55 | loss = 5.458714 
iteration = 60 | loss = 5.129730 
iteration = 65 | loss = 5.231947 
iteration = 70 | loss = 4.993608 
iteration = 75 | loss = 5.218278 
iteration = 80 | loss = 5.467890 
iteration = 85 | loss = 4.872981 
iteration = 90 | loss = 5.208120 
iteration = 95 | loss = 5.348342 
iteration = 100 | loss = 5.235509 
iteration = 105 | loss = 5.278357 
iteration = 0 | loss = 4.836847 
iteration = 5 | loss = 5.147024 
iteration = 10 | loss = 5.126055 
iteration = 15 | loss = 4.929296 
iteration = 20 | loss = 4.883265 
iteration = 25 | loss = 5.026081 
iteration = 30 | loss = 4.990075 
iteration = 35 | loss = 5.113202 
iteration = 40 | loss = 4.670427 
iteration = 45 | loss = 5.138439 
iteration = 50 | loss = 5.392545 
iteration = 55 | loss = 5.479236 
iteration = 60 | loss = 5.419024 
iteration = 65 | loss = 4.770929 
iteration = 70 | loss = 4.531451 
iteration = 75 | loss = 5.040730 
iteration = 80 | loss = 5.204635 
iteration = 85 | loss = 5.349186 
iteration = 90 | loss = 5.102998 
iteration = 95 | loss = 5.030059 
iteration = 100 | loss = 5.204936 
iteration = 105 | loss = 5.103177 
iteration = 0 | loss = 4.859086 
iteration = 5 | loss = 5.174694 
iteration = 10 | loss = 4.962042 
iteration = 15 | loss = 5.129116 
iteration = 20 | loss = 5.190250 
iteration = 25 | loss = 4.927180 
iteration = 30 | loss = 5.052106 
iteration = 35 | loss = 5.063473 
iteration = 40 | loss = 4.608677 
iteration = 45 | loss = 5.004546 
iteration = 50 | loss = 4.921457 
iteration = 55 | loss = 5.266284 
iteration = 60 | loss = 5.340936 
iteration = 65 | loss = 4.802378 
iteration = 70 | loss = 5.262123 
iteration = 75 | loss = 5.340903 
iteration = 80 | loss = 5.415277 
iteration = 85 | loss = 5.144682 
iteration = 90 | loss = 5.113984 
iteration = 95 | loss = 4.835033 
iteration = 100 | loss = 4.961642 
iteration = 105 | loss = 4.835808 
iteration = 0 | loss = 5.069039 
iteration = 5 | loss = 5.299787 
iteration = 10 | loss = 4.508338 
iteration = 15 | loss = 4.740153 
iteration = 20 | loss = 4.772388 
iteration = 25 | loss = 5.237367 
iteration = 30 | loss = 5.031981 
iteration = 35 | loss = 5.212174 
iteration = 40 | loss = 5.103438 
iteration = 45 | loss = 5.077430 
iteration = 50 | loss = 5.068090 
iteration = 55 | loss = 5.141388 
iteration = 60 | loss = 4.898341 
iteration = 65 | loss = 5.035933 
iteration = 70 | loss = 4.971007 
iteration = 75 | loss = 4.815654 
iteration = 80 | loss = 4.880527 
iteration = 85 | loss = 5.518406 
iteration = 90 | loss = 5.068537 
iteration = 95 | loss = 5.194578 
iteration = 100 | loss = 4.859477 
iteration = 105 | loss = 5.323699 
 Epoch: [20] Loss: 5.1743  R1_I2A: 0.3758 R1_A2I: 0.2970 
                 
iteration = 0 | loss = 5.077128 
iteration = 5 | loss = 4.833243 
iteration = 10 | loss = 4.731412 
iteration = 15 | loss = 4.833348 
iteration = 20 | loss = 5.369801 
iteration = 25 | loss = 4.791784 
iteration = 30 | loss = 5.271244 
iteration = 35 | loss = 4.515916 
iteration = 40 | loss = 4.672138 
iteration = 45 | loss = 5.316081 
iteration = 50 | loss = 5.194354 
iteration = 55 | loss = 4.917986 
iteration = 60 | loss = 5.268171 
iteration = 65 | loss = 5.158854 
iteration = 70 | loss = 5.164021 
iteration = 75 | loss = 5.109186 
iteration = 80 | loss = 5.168056 
iteration = 85 | loss = 4.704013 
iteration = 90 | loss = 5.239675 
iteration = 95 | loss = 5.308170 
iteration = 100 | loss = 4.989959 
iteration = 105 | loss = 5.518275 
iteration = 0 | loss = 4.763511 
iteration = 5 | loss = 5.046661 
iteration = 10 | loss = 5.142805 
iteration = 15 | loss = 4.928135 
iteration = 20 | loss = 4.764463 
iteration = 25 | loss = 4.981535 
iteration = 30 | loss = 5.179564 
iteration = 35 | loss = 4.267970 
iteration = 40 | loss = 5.046210 
iteration = 45 | loss = 4.889562 
iteration = 50 | loss = 5.072063 
iteration = 55 | loss = 4.836492 
iteration = 60 | loss = 4.677726 
iteration = 65 | loss = 5.285923 
iteration = 70 | loss = 4.818422 
iteration = 75 | loss = 4.884871 
iteration = 80 | loss = 4.900719 
iteration = 85 | loss = 4.949430 
iteration = 90 | loss = 4.959384 
iteration = 95 | loss = 5.135784 
iteration = 100 | loss = 4.899734 
iteration = 105 | loss = 4.930253 
iteration = 0 | loss = 4.889865 
iteration = 5 | loss = 5.745341 
iteration = 10 | loss = 5.237134 
iteration = 15 | loss = 4.925718 
iteration = 20 | loss = 5.205316 
iteration = 25 | loss = 5.161435 
iteration = 30 | loss = 5.304825 
iteration = 35 | loss = 5.246091 
iteration = 40 | loss = 4.818883 
iteration = 45 | loss = 5.026593 
iteration = 50 | loss = 4.879471 
iteration = 55 | loss = 5.036980 
iteration = 60 | loss = 5.742818 
iteration = 65 | loss = 4.819742 
iteration = 70 | loss = 4.578326 
iteration = 75 | loss = 5.147443 
iteration = 80 | loss = 5.428463 
iteration = 85 | loss = 5.011823 
iteration = 90 | loss = 4.534425 
slurmstepd: error: *** STEP 5155462.0 ON insy6 CANCELLED AT 2020-04-29T17:54:29 DUE TO TIME LIMIT ***
