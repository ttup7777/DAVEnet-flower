{'cmvn': False, 'margin': 0.5, 'max_length': 640, 'DATASET_NAME': 'flowers', 'DATASET_ALL_CLSS_NUM': 102, 'DATASET_TRAIN_CLSS_NUM': 82, 'CONFIG_NAME': '', 'DATA_DIR': '/tudelft.net/staff-bulk/ewi/insy/SpeechLab/TianTian/data/flowers/Oxford102', 'GPU_ID': 0, 'CUDA': True, 'WORKERS': 8, 'result_file': 'full.text', 'start_epoch': 0, 'RNN_TYPE': 'GRU', 'B_VALIDATION': False, 'image_attention': True, 'audio_attention': True, 'add_noise': False, 'TREE': {'BRANCH_NUM': 3, 'BASE_SIZE': 128}, 'TRAIN': {'MODAL': 'co-train', 'BATCH_SIZE': 64, 'MAX_EPOCH': 40, 'SNAPSHOT_INTERVAL': 2000, 'DISCRIMINATOR_LR': 0.0002, 'GENERATOR_LR': 0.0002, 'ENCODER_LR': 0.0002, 'RNN_GRAD_CLIP': 0.25, 'FLAG': True, 'NET_E': '', 'NET_G': '', 'B_NET_D': True, 'SMOOTH': {'GAMMA1': 5.0, 'GAMMA3': 13.0, 'GAMMA2': 5.0, 'LAMBDA': 1.0, 'IMGATT': 1.0, 'IMGATT2': 1.0}}, 'EXTRACT': {'split': 'train'}, 'CROSS_ATT': {'att': False, 'act': 'sigmoid', 'smooth_soft': 1.0, 'smooth_sigm': 0.1}, 'GAN': {'DF_DIM': 64, 'GF_DIM': 128, 'Z_DIM': 100, 'CONDITION_DIM': 100, 'R_NUM': 2, 'B_ATTENTION': True, 'B_DCGAN': False}, 'TEXT': {'CAPTIONS_PER_IMAGE': 10, 'EMBEDDING_DIM': 256, 'WORDS_NUM': 18}, 'IMG': {'style': 'raw'}, 'IMGF': {'Layer': 1, 'input_dim': 2048, 'hid_dim': 1600, 'embedding_dim': 1024}, 'rnn_type': 'LSTM', 'SPEECH': {'style': 'mel', 'model': 'CRNN', 'self_att': True, 'CAPTIONS_PER_IMAGE': 10, 'window_size': 25, 'stride': 10, 'input_dim': 40, 'hidden_size': 512, 'embedding_dim': 1024, 'num_layers': 2, 'sample': 22050, 'cmvn': True}, 'CNNRNN': {'rnn_type': 'GRU', 'in_channels': 40, 'hid_channels': 64, 'hid2_channels': 64, 'out_channels': 128, 'kernel_size': 6, 'stride': 2, 'padding': 0}, 'CNNRNN_RNN': {'input_size': 128, 'hidden_size': 512, 'num_layers': 2, 'dropout': 0.0, 'bidirectional': True}, 'CNNRNN_ATT': {'in_size': 1024, 'hidden_size': 128, 'n_heads': 1}, 'RNN': {'input_size': 40, 'hidden_size': 1024, 'num_layers': 3, 'dropout': 0.0, 'bidirectional': True}, 'RNN_ATT': {'in_size': 2048, 'hidden_size': 128, 'n_heads': 5}, 'CLASSIFICATION': {'data': 'audio'}, 'EVALUATE': {'dist': 'cosine'}, 'Loss': {'clss': True, 'cont': False, 'hinge': False, 'batch': True, 'KL': False, 'deco': False, 'adv': False, 'trip': False, 'dist': False, 'divers': False, 'batch_loc': False, 'disc': True, 'gamma_disc': 1.0, 'gamma_batch_loc': 1.0, 'gamma_divers': 1.0, 'gamma_clss': 1.0, 'gamma_cont': 1.0, 'gamma_hinge': 1.0, 'gamma_batch': 1.0, 'gamma_KL': 1.0, 'gamma_deco': 1.0, 'gamma_adv': 1.0, 'hinge_margin': 1.0, 'gamma_trip': 1.0, 'trip_margin': 1.0, 'gamma_dist': 1.0, 'dist_T': 1.0, 'adv_k': 5}, 'TEST': {'topk': 3}, 'exp_dir': 'outputs/01_Baseline/flowers/full_13/pre-train'}
Load filenames from: /tudelft.net/staff-bulk/ewi/insy/SpeechLab/TianTian/data/flowers/Oxford102/train/filenames.pickle (7034)
Load filenames from: /tudelft.net/staff-bulk/ewi/insy/SpeechLab/TianTian/data/flowers/Oxford102/test/filenames.pickle (1155)
Load filenames from: /tudelft.net/staff-bulk/ewi/insy/SpeechLab/TianTian/data/flowers/Oxford102/test/filenames.pickle (1155)
current #steps=0, #epochs=0
start training...
/tudelft.net/staff-bulk/ewi/insy/SpeechLab/TianTian/xinsheng/Retrieval_v4.3/utils/config.py:235: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  yaml_cfg = edict(yaml.load(f))
/tudelft.net/staff-bulk/ewi/insy/SpeechLab/TianTian/xinsheng/Retrieval_v4.3/models/AudioModels.py:89: UserWarning: nn.init.orthogonal is now deprecated in favor of nn.init.orthogonal_.
  nn.init.orthogonal(self.hidden.weight.data)
/tudelft.net/staff-bulk/ewi/insy/SpeechLab/TianTian/xinsheng/Retrieval_v4.3/models/AudioModels.py:91: UserWarning: nn.init.orthogonal is now deprecated in favor of nn.init.orthogonal_.
  nn.init.orthogonal(self.hidden.weight.data)
iteration = 0 | loss = 8.328390 
iteration = 5 | loss = 8.472100 
iteration = 10 | loss = 8.338551 
iteration = 15 | loss = 8.354253 
iteration = 20 | loss = 8.357392 
iteration = 25 | loss = 8.308607 
iteration = 30 | loss = 8.273335 
iteration = 35 | loss = 8.184506 
iteration = 40 | loss = 8.213751 
iteration = 45 | loss = 8.029030 
iteration = 50 | loss = 8.219296 
iteration = 55 | loss = 7.857326 
iteration = 60 | loss = 7.880337 
iteration = 65 | loss = 7.824053 
iteration = 70 | loss = 7.809358 
iteration = 75 | loss = 7.908288 
iteration = 80 | loss = 7.285586 
iteration = 85 | loss = 7.320619 
iteration = 90 | loss = 7.103049 
iteration = 95 | loss = 6.833477 
iteration = 100 | loss = 7.441748 
iteration = 105 | loss = 7.027258 
/home/nfs/tiantian/.local/lib/python3.6/site-packages/torch/nn/functional.py:1340: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.
  warnings.warn("nn.functional.tanh is deprecated. Use torch.tanh instead.")
iteration = 0 | loss = 6.987423 
iteration = 5 | loss = 7.238351 
iteration = 10 | loss = 6.920064 
iteration = 15 | loss = 6.572163 
iteration = 20 | loss = 7.039058 
iteration = 25 | loss = 6.561451 
iteration = 30 | loss = 6.633424 
iteration = 35 | loss = 6.355977 
iteration = 40 | loss = 6.527604 
iteration = 45 | loss = 6.047662 
iteration = 50 | loss = 6.792992 
iteration = 55 | loss = 6.408171 
iteration = 60 | loss = 6.848687 
iteration = 65 | loss = 6.877287 
iteration = 70 | loss = 6.136093 
iteration = 75 | loss = 6.579062 
iteration = 80 | loss = 5.783886 
iteration = 85 | loss = 6.095249 
iteration = 90 | loss = 6.052966 
iteration = 95 | loss = 6.255997 
iteration = 100 | loss = 5.714463 
iteration = 105 | loss = 5.781743 
iteration = 0 | loss = 5.737424 
iteration = 5 | loss = 5.844626 
iteration = 10 | loss = 5.822716 
iteration = 15 | loss = 6.167752 
iteration = 20 | loss = 5.920005 
iteration = 25 | loss = 5.605175 
iteration = 30 | loss = 5.833753 
iteration = 35 | loss = 5.745284 
iteration = 40 | loss = 5.899321 
iteration = 45 | loss = 5.251999 
iteration = 50 | loss = 5.737320 
iteration = 55 | loss = 5.693734 
iteration = 60 | loss = 5.196432 
iteration = 65 | loss = 5.458255 
iteration = 70 | loss = 5.311657 
iteration = 75 | loss = 5.241580 
iteration = 80 | loss = 5.228048 
iteration = 85 | loss = 6.312985 
iteration = 90 | loss = 5.185959 
iteration = 95 | loss = 5.703124 
iteration = 100 | loss = 6.083084 
iteration = 105 | loss = 5.852237 
iteration = 0 | loss = 5.520481 
iteration = 5 | loss = 5.520602 
iteration = 10 | loss = 5.059680 
iteration = 15 | loss = 5.213060 
iteration = 20 | loss = 5.354763 
iteration = 25 | loss = 5.279264 
iteration = 30 | loss = 5.562073 
iteration = 35 | loss = 4.751125 
iteration = 40 | loss = 5.507925 
iteration = 45 | loss = 4.792769 
iteration = 50 | loss = 5.275483 
iteration = 55 | loss = 5.330743 
iteration = 60 | loss = 4.965157 
iteration = 65 | loss = 4.919060 
iteration = 70 | loss = 5.489690 
iteration = 75 | loss = 5.188401 
iteration = 80 | loss = 5.113267 
iteration = 85 | loss = 4.892961 
iteration = 90 | loss = 4.197142 
iteration = 95 | loss = 4.956156 
iteration = 100 | loss = 5.409307 
iteration = 105 | loss = 4.984765 
iteration = 0 | loss = 4.978704 
iteration = 5 | loss = 4.968193 
iteration = 10 | loss = 4.894240 
iteration = 15 | loss = 4.865256 
iteration = 20 | loss = 4.434666 
iteration = 25 | loss = 4.424191 
iteration = 30 | loss = 4.594759 
iteration = 35 | loss = 4.432364 
iteration = 40 | loss = 5.088723 
iteration = 45 | loss = 4.788376 
iteration = 50 | loss = 4.803321 
iteration = 55 | loss = 5.047517 
iteration = 60 | loss = 4.703771 
iteration = 65 | loss = 5.112755 
iteration = 70 | loss = 5.326771 
iteration = 75 | loss = 5.037285 
iteration = 80 | loss = 5.229099 
iteration = 85 | loss = 4.480526 
iteration = 90 | loss = 4.781145 
iteration = 95 | loss = 4.471160 
iteration = 100 | loss = 4.421947 
iteration = 105 | loss = 4.983087 
 Epoch: [5] Loss: 4.8924  R1_I2A: 0.4857 R1_A2I: 0.3410 
                 
iteration = 0 | loss = 4.459778 
iteration = 5 | loss = 4.210702 
iteration = 10 | loss = 5.162209 
iteration = 15 | loss = 4.277595 
iteration = 20 | loss = 4.412440 
iteration = 25 | loss = 4.925656 
iteration = 30 | loss = 4.716911 
iteration = 35 | loss = 4.780753 
iteration = 40 | loss = 4.786965 
iteration = 45 | loss = 4.344001 
iteration = 50 | loss = 4.877789 
iteration = 55 | loss = 5.592362 
iteration = 60 | loss = 4.476750 
iteration = 65 | loss = 4.711973 
iteration = 70 | loss = 4.882731 
iteration = 75 | loss = 4.681021 
iteration = 80 | loss = 4.628223 
iteration = 85 | loss = 4.425224 
iteration = 90 | loss = 4.659437 
iteration = 95 | loss = 4.443268 
iteration = 100 | loss = 5.191430 
iteration = 105 | loss = 4.799732 
iteration = 0 | loss = 4.196375 
iteration = 5 | loss = 4.148584 
iteration = 10 | loss = 4.402505 
iteration = 15 | loss = 4.144100 
iteration = 20 | loss = 4.554584 
iteration = 25 | loss = 4.575603 
iteration = 30 | loss = 3.997723 
iteration = 35 | loss = 4.242673 
iteration = 40 | loss = 5.076513 
iteration = 45 | loss = 4.452934 
iteration = 50 | loss = 4.820679 
iteration = 55 | loss = 3.977600 
iteration = 60 | loss = 4.254426 
iteration = 65 | loss = 4.633003 
iteration = 70 | loss = 4.850821 
iteration = 75 | loss = 4.451200 
iteration = 80 | loss = 4.395621 
iteration = 85 | loss = 4.793949 
iteration = 90 | loss = 4.865100 
iteration = 95 | loss = 4.755793 
iteration = 100 | loss = 4.370106 
iteration = 105 | loss = 4.991565 
iteration = 0 | loss = 4.077909 
iteration = 5 | loss = 4.362638 
iteration = 10 | loss = 4.780083 
iteration = 15 | loss = 4.018576 
iteration = 20 | loss = 3.947939 
iteration = 25 | loss = 4.513081 
iteration = 30 | loss = 4.322235 
iteration = 35 | loss = 4.588672 
iteration = 40 | loss = 4.215419 
iteration = 45 | loss = 4.505612 
iteration = 50 | loss = 4.539037 
iteration = 55 | loss = 4.805336 
iteration = 60 | loss = 3.946611 
iteration = 65 | loss = 4.065481 
iteration = 70 | loss = 4.335399 
iteration = 75 | loss = 4.551661 
iteration = 80 | loss = 4.354081 
iteration = 85 | loss = 4.116843 
iteration = 90 | loss = 4.137215 
iteration = 95 | loss = 4.324514 
iteration = 100 | loss = 4.432459 
iteration = 105 | loss = 4.141639 
iteration = 0 | loss = 4.069979 
iteration = 5 | loss = 4.064871 
iteration = 10 | loss = 3.828743 
iteration = 15 | loss = 4.250496 
iteration = 20 | loss = 4.187702 
iteration = 25 | loss = 4.896249 
iteration = 30 | loss = 4.066113 
iteration = 35 | loss = 4.407818 
iteration = 40 | loss = 3.651173 
iteration = 45 | loss = 3.864223 
iteration = 50 | loss = 4.346249 
iteration = 55 | loss = 4.192945 
iteration = 60 | loss = 4.607779 
iteration = 65 | loss = 4.419291 
iteration = 70 | loss = 4.875519 
iteration = 75 | loss = 3.868799 
iteration = 80 | loss = 4.533736 
iteration = 85 | loss = 4.492085 
iteration = 90 | loss = 4.378426 
iteration = 95 | loss = 4.038255 
iteration = 100 | loss = 4.176591 
iteration = 105 | loss = 3.698961 
iteration = 0 | loss = 3.478460 
iteration = 5 | loss = 4.583116 
iteration = 10 | loss = 4.511545 
iteration = 15 | loss = 3.791172 
iteration = 20 | loss = 3.959267 
iteration = 25 | loss = 3.633895 
iteration = 30 | loss = 4.376254 
iteration = 35 | loss = 4.046939 
iteration = 40 | loss = 4.233871 
iteration = 45 | loss = 4.644621 
iteration = 50 | loss = 4.021201 
iteration = 55 | loss = 4.327295 
iteration = 60 | loss = 3.976673 
iteration = 65 | loss = 4.366521 
iteration = 70 | loss = 4.579062 
iteration = 75 | loss = 3.511458 
iteration = 80 | loss = 4.309742 
iteration = 85 | loss = 4.326549 
iteration = 90 | loss = 4.402472 
iteration = 95 | loss = 4.372837 
iteration = 100 | loss = 4.288480 
iteration = 105 | loss = 3.745567 
 Epoch: [10] Loss: 4.1498  R1_I2A: 0.5429 R1_A2I: 0.3998 
                 
iteration = 0 | loss = 4.349031 
iteration = 5 | loss = 3.958196 
iteration = 10 | loss = 4.373507 
iteration = 15 | loss = 4.504951 
iteration = 20 | loss = 4.230717 
iteration = 25 | loss = 4.208391 
iteration = 30 | loss = 3.654487 
iteration = 35 | loss = 3.550079 
iteration = 40 | loss = 4.150289 
iteration = 45 | loss = 4.436940 
iteration = 50 | loss = 4.164466 
iteration = 55 | loss = 4.034265 
iteration = 60 | loss = 4.065628 
iteration = 65 | loss = 4.055726 
iteration = 70 | loss = 4.034141 
iteration = 75 | loss = 3.308001 
iteration = 80 | loss = 4.290246 
iteration = 85 | loss = 3.912800 
iteration = 90 | loss = 3.753125 
iteration = 95 | loss = 3.869603 
iteration = 100 | loss = 3.833672 
iteration = 105 | loss = 3.870302 
iteration = 0 | loss = 4.231750 
iteration = 5 | loss = 4.285726 
iteration = 10 | loss = 4.035335 
iteration = 15 | loss = 3.991506 
iteration = 20 | loss = 3.956966 
iteration = 25 | loss = 4.102449 
iteration = 30 | loss = 4.115484 
iteration = 35 | loss = 3.405511 
iteration = 40 | loss = 4.457949 
iteration = 45 | loss = 4.240565 
iteration = 50 | loss = 3.998523 
iteration = 55 | loss = 3.615001 
iteration = 60 | loss = 3.569152 
iteration = 65 | loss = 3.761293 
iteration = 70 | loss = 3.532218 
iteration = 75 | loss = 4.110529 
iteration = 80 | loss = 3.914228 
iteration = 85 | loss = 3.416459 
iteration = 90 | loss = 3.314853 
iteration = 95 | loss = 3.664684 
iteration = 100 | loss = 4.026525 
iteration = 105 | loss = 3.918367 
iteration = 0 | loss = 4.236042 
iteration = 5 | loss = 3.543845 
iteration = 10 | loss = 4.267097 
iteration = 15 | loss = 3.408706 
iteration = 20 | loss = 3.575145 
iteration = 25 | loss = 3.552572 
iteration = 30 | loss = 4.024679 
iteration = 35 | loss = 4.436357 
iteration = 40 | loss = 3.883881 
iteration = 45 | loss = 4.075434 
iteration = 50 | loss = 3.818801 
iteration = 55 | loss = 3.901306 
iteration = 60 | loss = 3.607431 
iteration = 65 | loss = 3.758777 
iteration = 70 | loss = 3.458747 
iteration = 75 | loss = 3.637393 
iteration = 80 | loss = 4.533780 
iteration = 85 | loss = 3.855104 
iteration = 90 | loss = 3.751758 
iteration = 95 | loss = 3.319483 
iteration = 100 | loss = 3.942313 
iteration = 105 | loss = 3.323604 
iteration = 0 | loss = 4.170866 
iteration = 5 | loss = 3.117699 
iteration = 10 | loss = 4.142563 
iteration = 15 | loss = 3.830722 
iteration = 20 | loss = 3.663727 
iteration = 25 | loss = 3.792774 
iteration = 30 | loss = 3.742900 
iteration = 35 | loss = 3.537771 
iteration = 40 | loss = 3.868153 
iteration = 45 | loss = 3.488123 
iteration = 50 | loss = 3.109347 
iteration = 55 | loss = 3.491920 
iteration = 60 | loss = 4.533056 
iteration = 65 | loss = 3.669212 
iteration = 70 | loss = 3.515218 
iteration = 75 | loss = 3.833608 
iteration = 80 | loss = 4.124805 
iteration = 85 | loss = 3.554764 
iteration = 90 | loss = 3.910419 
iteration = 95 | loss = 4.117597 
iteration = 100 | loss = 3.253687 
iteration = 105 | loss = 3.299237 
iteration = 0 | loss = 4.242654 
iteration = 5 | loss = 3.183505 
iteration = 10 | loss = 3.896707 
iteration = 15 | loss = 3.213118 
iteration = 20 | loss = 3.870870 
iteration = 25 | loss = 4.186861 
iteration = 30 | loss = 3.384364 
iteration = 35 | loss = 3.405433 
iteration = 40 | loss = 3.361511 
iteration = 45 | loss = 3.997815 
iteration = 50 | loss = 4.061238 
iteration = 55 | loss = 3.227758 
iteration = 60 | loss = 3.875925 
iteration = 65 | loss = 3.810795 
iteration = 70 | loss = 3.822677 
iteration = 75 | loss = 4.485330 
iteration = 80 | loss = 4.114083 
iteration = 85 | loss = 3.256599 
iteration = 90 | loss = 3.088915 
iteration = 95 | loss = 3.630010 
iteration = 100 | loss = 3.762590 
iteration = 105 | loss = 3.825227 
 Epoch: [15] Loss: 4.1247  R1_I2A: 0.5792 R1_A2I: 0.4227 
                 
iteration = 0 | loss = 4.341010 
iteration = 5 | loss = 3.924941 
iteration = 10 | loss = 3.516057 
iteration = 15 | loss = 3.662519 
iteration = 20 | loss = 3.262969 
iteration = 25 | loss = 3.755871 
iteration = 30 | loss = 3.555975 
iteration = 35 | loss = 4.310949 
iteration = 40 | loss = 3.457110 
iteration = 45 | loss = 4.015252 
iteration = 50 | loss = 3.707763 
iteration = 55 | loss = 3.392281 
iteration = 60 | loss = 3.781343 
iteration = 65 | loss = 4.207128 
iteration = 70 | loss = 3.672078 
iteration = 75 | loss = 3.835223 
iteration = 80 | loss = 3.217300 
iteration = 85 | loss = 3.623737 
iteration = 90 | loss = 3.037967 
iteration = 95 | loss = 3.499352 
iteration = 100 | loss = 3.754475 
iteration = 105 | loss = 3.308356 
iteration = 0 | loss = 3.225316 
iteration = 5 | loss = 4.219543 
iteration = 10 | loss = 3.063048 
iteration = 15 | loss = 3.333286 
iteration = 20 | loss = 3.543659 
iteration = 25 | loss = 3.510106 
iteration = 30 | loss = 3.899563 
iteration = 35 | loss = 3.641312 
iteration = 40 | loss = 3.739078 
iteration = 45 | loss = 3.224812 
iteration = 50 | loss = 3.137043 
iteration = 55 | loss = 3.329785 
iteration = 60 | loss = 3.961491 
iteration = 65 | loss = 3.119231 
iteration = 70 | loss = 3.427530 
iteration = 75 | loss = 3.808637 
iteration = 80 | loss = 3.072102 
iteration = 85 | loss = 4.030125 
iteration = 90 | loss = 3.518312 
iteration = 95 | loss = 3.482370 
iteration = 100 | loss = 3.845143 
iteration = 105 | loss = 3.449689 
iteration = 0 | loss = 2.955828 
iteration = 5 | loss = 2.906568 
iteration = 10 | loss = 3.069365 
iteration = 15 | loss = 3.636150 
iteration = 20 | loss = 3.110885 
iteration = 25 | loss = 3.766864 
iteration = 30 | loss = 3.419424 
iteration = 35 | loss = 4.223552 
iteration = 40 | loss = 3.663137 
iteration = 45 | loss = 3.164022 
iteration = 50 | loss = 3.791745 
iteration = 55 | loss = 3.972588 
iteration = 60 | loss = 3.671001 
iteration = 65 | loss = 3.111897 
iteration = 70 | loss = 3.098995 
iteration = 75 | loss = 3.518684 
iteration = 80 | loss = 3.627840 
iteration = 85 | loss = 3.259764 
iteration = 90 | loss = 3.641508 
iteration = 95 | loss = 3.176665 
iteration = 100 | loss = 3.485451 
iteration = 105 | loss = 2.939639 
iteration = 0 | loss = 3.615584 
iteration = 5 | loss = 3.167050 
iteration = 10 | loss = 3.349489 
iteration = 15 | loss = 3.767286 
iteration = 20 | loss = 3.433917 
iteration = 25 | loss = 3.467438 
iteration = 30 | loss = 3.179834 
iteration = 35 | loss = 4.010673 
iteration = 40 | loss = 4.142856 
iteration = 45 | loss = 3.070750 
iteration = 50 | loss = 3.013649 
iteration = 55 | loss = 3.435492 
iteration = 60 | loss = 3.468459 
iteration = 65 | loss = 2.990820 
iteration = 70 | loss = 2.842647 
iteration = 75 | loss = 3.656683 
iteration = 80 | loss = 3.678249 
iteration = 85 | loss = 3.256088 
iteration = 90 | loss = 3.745599 
iteration = 95 | loss = 3.325599 
iteration = 100 | loss = 3.553509 
iteration = 105 | loss = 3.397138 
iteration = 0 | loss = 3.343404 
iteration = 5 | loss = 3.524606 
iteration = 10 | loss = 3.421497 
iteration = 15 | loss = 3.528574 
iteration = 20 | loss = 3.477355 
iteration = 25 | loss = 3.455131 
iteration = 30 | loss = 2.977130 
iteration = 35 | loss = 2.813920 
iteration = 40 | loss = 2.974457 
iteration = 45 | loss = 3.671313 
iteration = 50 | loss = 3.429100 
iteration = 55 | loss = 3.108813 
iteration = 60 | loss = 3.073526 
iteration = 65 | loss = 3.299851 
iteration = 70 | loss = 3.622337 
iteration = 75 | loss = 3.350835 
iteration = 80 | loss = 3.403720 
iteration = 85 | loss = 3.113239 
iteration = 90 | loss = 3.279408 
iteration = 95 | loss = 3.630806 
iteration = 100 | loss = 3.179846 
iteration = 105 | loss = 3.397548 
 Epoch: [20] Loss: 3.3321  R1_I2A: 0.5870 R1_A2I: 0.4377 
                 
iteration = 0 | loss = 3.435855 
iteration = 5 | loss = 3.780145 
iteration = 10 | loss = 3.375929 
iteration = 15 | loss = 3.862604 
iteration = 20 | loss = 3.220426 
iteration = 25 | loss = 3.529892 
iteration = 30 | loss = 3.376140 
iteration = 35 | loss = 3.034019 
iteration = 40 | loss = 3.248063 
iteration = 45 | loss = 3.865413 
iteration = 50 | loss = 3.285136 
iteration = 55 | loss = 3.406661 
iteration = 60 | loss = 3.003188 
iteration = 65 | loss = 3.913276 
iteration = 70 | loss = 3.163547 
iteration = 75 | loss = 3.631744 
iteration = 80 | loss = 2.978800 
iteration = 85 | loss = 2.826901 
iteration = 90 | loss = 3.406078 
iteration = 95 | loss = 3.229346 
iteration = 100 | loss = 2.864758 
iteration = 105 | loss = 3.515624 
iteration = 0 | loss = 2.998149 
iteration = 5 | loss = 3.422641 
iteration = 10 | loss = 3.031414 
iteration = 15 | loss = 3.439807 
iteration = 20 | loss = 3.217958 
iteration = 25 | loss = 3.362037 
iteration = 30 | loss = 3.275929 
iteration = 35 | loss = 2.880886 
iteration = 40 | loss = 3.031769 
iteration = 45 | loss = 3.088441 
iteration = 50 | loss = 3.524161 
iteration = 55 | loss = 3.275444 
iteration = 60 | loss = 3.127402 
iteration = 65 | loss = 3.526153 
iteration = 70 | loss = 3.584340 
iteration = 75 | loss = 3.156772 
iteration = 80 | loss = 3.360725 
iteration = 85 | loss = 2.950879 
iteration = 90 | loss = 2.754060 
iteration = 95 | loss = 3.006640 
iteration = 100 | loss = 3.578887 
iteration = 105 | loss = 2.999964 
iteration = 0 | loss = 3.363254 
iteration = 5 | loss = 2.975735 
iteration = 10 | loss = 3.327117 
iteration = 15 | loss = 3.263016 
iteration = 20 | loss = 3.133680 
iteration = 25 | loss = 2.459057 
iteration = 30 | loss = 2.962442 
iteration = 35 | loss = 3.523448 
iteration = 40 | loss = 3.427313 
iteration = 45 | loss = 3.220346 
iteration = 50 | loss = 3.223502 
iteration = 55 | loss = 3.256136 
iteration = 60 | loss = 3.224546 
iteration = 65 | loss = 3.238699 
iteration = 70 | loss = 3.309109 
iteration = 75 | loss = 3.167304 
iteration = 80 | loss = 2.871913 
iteration = 85 | loss = 3.079844 
iteration = 90 | loss = 3.273130 
iteration = 95 | loss = 2.817699 
iteration = 100 | loss = 3.852695 
iteration = 105 | loss = 2.790787 
iteration = 0 | loss = 3.219687 
iteration = 5 | loss = 3.262915 
iteration = 10 | loss = 3.251586 
iteration = 15 | loss = 2.678449 
iteration = 20 | loss = 3.166244 
iteration = 25 | loss = 3.385794 
iteration = 30 | loss = 3.298020 
iteration = 35 | loss = 3.191245 
iteration = 40 | loss = 2.798304 
iteration = 45 | loss = 3.075045 
iteration = 50 | loss = 3.185541 
iteration = 55 | loss = 2.873694 
iteration = 60 | loss = 2.669558 
iteration = 65 | loss = 3.016740 
iteration = 70 | loss = 3.335857 
iteration = 75 | loss = 3.309408 
iteration = 80 | loss = 3.531562 
iteration = 85 | loss = 3.643211 
iteration = 90 | loss = 3.702893 
iteration = 95 | loss = 2.634072 
iteration = 100 | loss = 3.327613 
iteration = 105 | loss = 2.871936 
iteration = 0 | loss = 3.408090 
iteration = 5 | loss = 3.299956 
iteration = 10 | loss = 3.007649 
iteration = 15 | loss = 2.929915 
iteration = 20 | loss = 3.216517 
iteration = 25 | loss = 3.180160 
iteration = 30 | loss = 3.374326 
iteration = 35 | loss = 3.441901 
iteration = 40 | loss = 3.069806 
iteration = 45 | loss = 3.386955 
iteration = 50 | loss = 2.936293 
iteration = 55 | loss = 3.745696 
iteration = 60 | loss = 3.144942 
iteration = 65 | loss = 3.134686 
iteration = 70 | loss = 2.995495 
iteration = 75 | loss = 3.289557 
iteration = 80 | loss = 3.304178 
iteration = 85 | loss = 3.322298 
iteration = 90 | loss = 2.867999 
iteration = 95 | loss = 3.663928 
iteration = 100 | loss = 3.135948 
iteration = 105 | loss = 2.903428 
 Epoch: [25] Loss: 3.2859  R1_I2A: 0.5654 R1_A2I: 0.4429 
                 
iteration = 0 | loss = 3.227388 
iteration = 5 | loss = 3.273319 
iteration = 10 | loss = 3.160464 
iteration = 15 | loss = 3.175605 
iteration = 20 | loss = 3.052669 
iteration = 25 | loss = 3.187487 
iteration = 30 | loss = 3.263061 
iteration = 35 | loss = 3.139215 
iteration = 40 | loss = 2.827081 
iteration = 45 | loss = 2.985677 
iteration = 50 | loss = 3.303633 
iteration = 55 | loss = 3.591510 
iteration = 60 | loss = 3.339324 
iteration = 65 | loss = 3.263795 
iteration = 70 | loss = 3.106063 
iteration = 75 | loss = 3.154402 
iteration = 80 | loss = 2.879235 
iteration = 85 | loss = 3.292039 
iteration = 90 | loss = 3.177363 
iteration = 95 | loss = 3.092656 
iteration = 100 | loss = 3.131471 
iteration = 105 | loss = 3.013385 
iteration = 0 | loss = 3.359127 
iteration = 5 | loss = 3.330477 
iteration = 10 | loss = 2.930456 
iteration = 15 | loss = 3.683055 
iteration = 20 | loss = 3.083662 
iteration = 25 | loss = 2.827403 
iteration = 30 | loss = 2.767178 
iteration = 35 | loss = 3.199358 
iteration = 40 | loss = 3.371819 
iteration = 45 | loss = 2.776340 
iteration = 50 | loss = 3.781460 
iteration = 55 | loss = 2.633575 
iteration = 60 | loss = 2.166843 
iteration = 65 | loss = 3.165644 
iteration = 70 | loss = 2.815032 
iteration = 75 | loss = 2.961549 
iteration = 80 | loss = 3.360138 
iteration = 85 | loss = 3.000608 
iteration = 90 | loss = 3.075786 
iteration = 95 | loss = 3.789060 
iteration = 100 | loss = 3.207432 
iteration = 105 | loss = 2.945088 
iteration = 0 | loss = 2.542493 
iteration = 5 | loss = 2.461466 
iteration = 10 | loss = 2.532488 
iteration = 15 | loss = 3.475731 
iteration = 20 | loss = 2.902625 
iteration = 25 | loss = 2.980001 
iteration = 30 | loss = 2.901023 
iteration = 35 | loss = 2.916013 
iteration = 40 | loss = 3.087087 
iteration = 45 | loss = 3.291804 
iteration = 50 | loss = 3.098234 
iteration = 55 | loss = 2.753293 
iteration = 60 | loss = 2.984181 
iteration = 65 | loss = 3.092938 
iteration = 70 | loss = 3.351700 
iteration = 75 | loss = 3.152893 
iteration = 80 | loss = 2.564916 
iteration = 85 | loss = 2.950754 
iteration = 90 | loss = 3.063412 
iteration = 95 | loss = 3.422699 
iteration = 100 | loss = 2.491773 
iteration = 105 | loss = 3.053033 
iteration = 0 | loss = 3.003061 
iteration = 5 | loss = 3.287779 
iteration = 10 | loss = 3.358826 
iteration = 15 | loss = 3.082882 
iteration = 20 | loss = 3.305012 
iteration = 25 | loss = 3.057961 
iteration = 30 | loss = 3.045597 
iteration = 35 | loss = 2.800484 
iteration = 40 | loss = 3.771778 
iteration = 45 | loss = 3.316320 
iteration = 50 | loss = 3.458242 
iteration = 55 | loss = 3.325957 
iteration = 60 | loss = 2.573825 
iteration = 65 | loss = 3.299540 
iteration = 70 | loss = 3.300089 
iteration = 75 | loss = 3.589849 
iteration = 80 | loss = 2.815325 
iteration = 85 | loss = 2.626176 
iteration = 90 | loss = 2.998770 
iteration = 95 | loss = 2.986329 
iteration = 100 | loss = 2.797154 
iteration = 105 | loss = 3.081732 
iteration = 0 | loss = 3.111488 
iteration = 5 | loss = 3.387690 
iteration = 10 | loss = 3.030364 
iteration = 15 | loss = 3.445113 
iteration = 20 | loss = 2.794028 
iteration = 25 | loss = 3.646513 
iteration = 30 | loss = 2.816276 
iteration = 35 | loss = 2.946818 
iteration = 40 | loss = 3.035277 
iteration = 45 | loss = 2.873415 
iteration = 50 | loss = 3.065377 
iteration = 55 | loss = 3.927345 
iteration = 60 | loss = 2.898863 
iteration = 65 | loss = 3.249348 
iteration = 70 | loss = 3.409097 
iteration = 75 | loss = 2.261221 
iteration = 80 | loss = 2.967385 
iteration = 85 | loss = 3.217352 
iteration = 90 | loss = 2.792148 
iteration = 95 | loss = 3.059202 
iteration = 100 | loss = 3.030330 
iteration = 105 | loss = 3.312437 
 Epoch: [30] Loss: 3.1498  R1_I2A: 0.5662 R1_A2I: 0.4610 
                 
 Epoch: [30] Loss: 3.1498  R1_I2A: 0.5704 mAP_I2A: 0.5206  R1_A2I: 0.4610 mAP_A2I: 0.4051 
                     
iteration = 0 | loss = 3.026122 
iteration = 5 | loss = 2.844632 
iteration = 10 | loss = 3.013494 
iteration = 15 | loss = 2.531168 
iteration = 20 | loss = 3.152496 
iteration = 25 | loss = 3.199968 
iteration = 30 | loss = 2.942910 
iteration = 35 | loss = 2.795582 
iteration = 40 | loss = 2.574407 
iteration = 45 | loss = 2.792682 
iteration = 50 | loss = 2.601056 
iteration = 55 | loss = 3.230169 
iteration = 60 | loss = 3.466023 
iteration = 65 | loss = 2.970620 
iteration = 70 | loss = 2.777746 
iteration = 75 | loss = 2.737175 
iteration = 80 | loss = 2.928846 
iteration = 85 | loss = 2.824734 
iteration = 90 | loss = 3.314477 
iteration = 95 | loss = 3.206508 
iteration = 100 | loss = 2.997646 
iteration = 105 | loss = 3.136461 
iteration = 0 | loss = 2.924228 
iteration = 5 | loss = 2.937021 
iteration = 10 | loss = 3.672894 
iteration = 15 | loss = 2.915108 
iteration = 20 | loss = 2.957362 
iteration = 25 | loss = 3.134625 
iteration = 30 | loss = 3.119725 
iteration = 35 | loss = 3.679710 
iteration = 40 | loss = 2.799396 
iteration = 45 | loss = 2.516141 
iteration = 50 | loss = 2.747390 
iteration = 55 | loss = 2.971518 
iteration = 60 | loss = 2.687570 
iteration = 65 | loss = 3.475360 
iteration = 70 | loss = 3.169296 
iteration = 75 | loss = 2.781669 
iteration = 80 | loss = 3.126508 
iteration = 85 | loss = 2.183619 
iteration = 90 | loss = 2.956221 
iteration = 95 | loss = 2.860116 
iteration = 100 | loss = 2.576155 
iteration = 105 | loss = 2.970061 
iteration = 0 | loss = 3.120768 
iteration = 5 | loss = 3.278394 
iteration = 10 | loss = 3.173098 
iteration = 15 | loss = 3.364025 
iteration = 20 | loss = 2.598220 
iteration = 25 | loss = 2.788598 
iteration = 30 | loss = 2.753497 
iteration = 35 | loss = 2.828409 
iteration = 40 | loss = 2.539258 
iteration = 45 | loss = 2.918602 
iteration = 50 | loss = 2.820714 
iteration = 55 | loss = 3.281029 
iteration = 60 | loss = 3.001308 
iteration = 65 | loss = 2.916808 
iteration = 70 | loss = 3.348896 
iteration = 75 | loss = 3.084260 
iteration = 80 | loss = 2.867246 
iteration = 85 | loss = 3.183553 
iteration = 90 | loss = 3.010739 
iteration = 95 | loss = 2.799744 
iteration = 100 | loss = 2.728257 
iteration = 105 | loss = 2.922649 
iteration = 0 | loss = 2.855050 
iteration = 5 | loss = 2.703590 
iteration = 10 | loss = 2.404603 
iteration = 15 | loss = 2.565101 
iteration = 20 | loss = 3.348283 
iteration = 25 | loss = 2.583910 
iteration = 30 | loss = 2.704896 
iteration = 35 | loss = 2.714302 
iteration = 40 | loss = 2.878556 
iteration = 45 | loss = 2.600869 
iteration = 50 | loss = 3.493112 
iteration = 55 | loss = 2.759238 
iteration = 60 | loss = 3.052620 
iteration = 65 | loss = 2.929238 
iteration = 70 | loss = 2.875969 
iteration = 75 | loss = 2.166836 
iteration = 80 | loss = 3.349295 
iteration = 85 | loss = 2.778291 
iteration = 90 | loss = 3.015119 
iteration = 95 | loss = 3.647003 
iteration = 100 | loss = 2.967410 
iteration = 105 | loss = 3.075660 
iteration = 0 | loss = 2.948694 
iteration = 5 | loss = 2.521134 
iteration = 10 | loss = 2.622494 
iteration = 15 | loss = 2.670116 
iteration = 20 | loss = 2.986650 
iteration = 25 | loss = 2.615603 
iteration = 30 | loss = 2.972245 
iteration = 35 | loss = 3.368711 
iteration = 40 | loss = 2.647538 
iteration = 45 | loss = 2.616391 
iteration = 50 | loss = 2.461654 
iteration = 55 | loss = 2.771026 
iteration = 60 | loss = 2.365102 
iteration = 65 | loss = 2.454697 
iteration = 70 | loss = 2.747592 
iteration = 75 | loss = 2.405038 
iteration = 80 | loss = 2.581471 
iteration = 85 | loss = 2.784725 
iteration = 90 | loss = 2.673923 
iteration = 95 | loss = 2.784199 
iteration = 100 | loss = 2.667850 
iteration = 105 | loss = 3.153890 
 Epoch: [35] Loss: 2.6488  R1_I2A: 0.6095 R1_A2I: 0.4748 
                 
 Epoch: [35] Loss: 2.6488  R1_I2A: 0.6120 mAP_I2A: 0.5314  R1_A2I: 0.4748 mAP_A2I: 0.4153 
                     
iteration = 0 | loss = 3.292312 
iteration = 5 | loss = 3.289234 
iteration = 10 | loss = 2.891074 
iteration = 15 | loss = 3.226874 
iteration = 20 | loss = 2.510824 
iteration = 25 | loss = 2.855532 
iteration = 30 | loss = 2.657259 
iteration = 35 | loss = 2.120359 
iteration = 40 | loss = 2.555830 
iteration = 45 | loss = 2.424162 
iteration = 50 | loss = 2.618657 
iteration = 55 | loss = 2.424512 
iteration = 60 | loss = 3.204520 
iteration = 65 | loss = 2.680812 
iteration = 70 | loss = 3.020653 
iteration = 75 | loss = 2.995383 
iteration = 80 | loss = 2.662294 
iteration = 85 | loss = 2.274530 
iteration = 90 | loss = 2.787760 
iteration = 95 | loss = 2.773263 
iteration = 100 | loss = 3.239731 
iteration = 105 | loss = 2.836187 
iteration = 0 | loss = 2.593055 
iteration = 5 | loss = 3.344413 
iteration = 10 | loss = 2.980782 
iteration = 15 | loss = 2.230131 
iteration = 20 | loss = 2.733265 
iteration = 25 | loss = 2.269548 
iteration = 30 | loss = 2.627373 
iteration = 35 | loss = 2.504941 
iteration = 40 | loss = 3.066713 
iteration = 45 | loss = 2.616514 
iteration = 50 | loss = 2.980731 
iteration = 55 | loss = 2.592628 
iteration = 60 | loss = 2.631236 
iteration = 65 | loss = 2.668492 
iteration = 70 | loss = 2.793559 
iteration = 75 | loss = 2.432169 
iteration = 80 | loss = 2.640000 
iteration = 85 | loss = 2.749457 
iteration = 90 | loss = 2.505392 
iteration = 95 | loss = 2.530943 
iteration = 100 | loss = 2.455065 
iteration = 105 | loss = 2.832308 
iteration = 0 | loss = 2.945799 
iteration = 5 | loss = 2.819979 
iteration = 10 | loss = 2.735240 
iteration = 15 | loss = 2.596547 
iteration = 20 | loss = 2.499377 
iteration = 25 | loss = 2.869331 
iteration = 30 | loss = 3.254431 
iteration = 35 | loss = 2.472083 
iteration = 40 | loss = 2.137614 
iteration = 45 | loss = 2.566153 
iteration = 50 | loss = 2.365123 
iteration = 55 | loss = 2.916695 
iteration = 60 | loss = 2.773129 
iteration = 65 | loss = 2.859813 
iteration = 70 | loss = 2.773038 
iteration = 75 | loss = 2.757765 
iteration = 80 | loss = 2.524280 
iteration = 85 | loss = 2.872874 
iteration = 90 | loss = 2.888787 
iteration = 95 | loss = 2.625681 
iteration = 100 | loss = 2.744051 
iteration = 105 | loss = 2.616814 
iteration = 0 | loss = 3.097888 
iteration = 5 | loss = 2.460674 
iteration = 10 | loss = 2.162633 
iteration = 15 | loss = 3.362781 
iteration = 20 | loss = 2.461765 
iteration = 25 | loss = 2.950603 
iteration = 30 | loss = 3.084905 
iteration = 35 | loss = 2.566224 
iteration = 40 | loss = 3.070453 
iteration = 45 | loss = 2.798831 
iteration = 50 | loss = 2.955964 
iteration = 55 | loss = 2.750638 
iteration = 60 | loss = 3.060285 
iteration = 65 | loss = 2.294092 
iteration = 70 | loss = 2.840110 
iteration = 75 | loss = 2.375877 
iteration = 80 | loss = 2.290416 
iteration = 85 | loss = 3.252109 
iteration = 90 | loss = 2.665802 
iteration = 95 | loss = 2.599844 
iteration = 100 | loss = 2.758297 
iteration = 105 | loss = 2.862703 
iteration = 0 | loss = 2.482761 
iteration = 5 | loss = 2.989298 
iteration = 10 | loss = 2.976189 
iteration = 15 | loss = 2.993588 
iteration = 20 | loss = 2.917051 
iteration = 25 | loss = 2.778613 
iteration = 30 | loss = 2.649734 
iteration = 35 | loss = 2.475312 
iteration = 40 | loss = 2.346153 
iteration = 45 | loss = 2.836107 
iteration = 50 | loss = 2.657911 
iteration = 55 | loss = 2.872636 
iteration = 60 | loss = 3.306239 
iteration = 65 | loss = 2.875320 
iteration = 70 | loss = 2.938580 
iteration = 75 | loss = 2.630158 
iteration = 80 | loss = 2.596878 
iteration = 85 | loss = 2.500546 
iteration = 90 | loss = 2.735399 
iteration = 95 | loss = 3.013730 
iteration = 100 | loss = 2.839340 
iteration = 105 | loss = 2.669256 
 Epoch: [40] Loss: 2.6024  R1_I2A: 0.5481 R1_A2I: 0.4674 
                 
iteration = 0 | loss = 2.603771 
iteration = 5 | loss = 2.776490 
iteration = 10 | loss = 2.594718 
iteration = 15 | loss = 2.327878 
iteration = 20 | loss = 2.216294 
iteration = 25 | loss = 2.299837 
iteration = 30 | loss = 2.762714 
iteration = 35 | loss = 2.854245 
iteration = 40 | loss = 2.698290 
iteration = 45 | loss = 2.848593 
iteration = 50 | loss = 2.782156 
iteration = 55 | loss = 2.526135 
iteration = 60 | loss = 2.231380 
iteration = 65 | loss = 2.820754 
iteration = 70 | loss = 2.290410 
iteration = 75 | loss = 2.501678 
iteration = 80 | loss = 2.974739 
iteration = 85 | loss = 2.610272 
iteration = 90 | loss = 2.414454 
iteration = 95 | loss = 2.559705 
iteration = 100 | loss = 2.984158 
iteration = 105 | loss = 2.217578 
