Namespace(RNN_dropout=0.0, WORKERS=8, audio_attention=None, audio_model='Davenet', batch_size=64, cfg_file='Confg/flower_train_batch.yml', data_path='/tudelft.net/staff-bulk/ewi/insy/MMC/xinsheng/data/102flowers', exp_dir='', gamma_clss=1.0, gpu_id=0, image_attention=None, image_model='VGG16', img_size=256, loss_clss=None, lr=0.0001, lr_decay=50, manualSeed=200, margin=1.0, momentum=0.9, n_epochs=40, n_heads=1, n_print_steps=2, optim='adam', pretrained_image_model=False, result_file='full.text', resume=True, rnn_type='LSTM', save_root='outputs/01_Baseline/flowers/full', simtype='MISA', smooth_gamm3=10.0, smooth_imgatt1=1.0, smooth_imgatt2=1.0, start_epoch=0, tasks='extraction', topk=3, weight_decay=1e-05)
Load filenames from: /tudelft.net/staff-bulk/ewi/insy/MMC/xinsheng/data/102flowers/train/filenames.pickle (7034)
Load filenames from: /tudelft.net/staff-bulk/ewi/insy/MMC/xinsheng/data/102flowers/test/filenames.pickle (1155)
Load filenames from: /tudelft.net/staff-bulk/ewi/insy/MMC/xinsheng/data/102flowers/test/filenames.pickle (1155)
current #steps=0, #epochs=0
start training...
/tudelft.net/staff-bulk/ewi/insy/MMC/xinsheng/code/MSc/Tiant/Retrieval_v4.3/utils/config.py:235: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  yaml_cfg = edict(yaml.load(f))
/tudelft.net/staff-bulk/ewi/insy/MMC/xinsheng/code/MSc/Tiant/Retrieval_v4.3/models/AudioModels.py:89: UserWarning: nn.init.orthogonal is now deprecated in favor of nn.init.orthogonal_.
  nn.init.orthogonal(self.hidden.weight.data)
/tudelft.net/staff-bulk/ewi/insy/MMC/xinsheng/code/MSc/Tiant/Retrieval_v4.3/models/AudioModels.py:91: UserWarning: nn.init.orthogonal is now deprecated in favor of nn.init.orthogonal_.
  nn.init.orthogonal(self.hidden.weight.data)
iteration = 0 | loss = 8.310975 
iteration = 5 | loss = 8.387739 
iteration = 10 | loss = 8.300470 
iteration = 15 | loss = 8.311893 
iteration = 20 | loss = 8.310745 
iteration = 25 | loss = 8.256435 
iteration = 30 | loss = 8.215384 
iteration = 35 | loss = 8.122780 
iteration = 40 | loss = 8.125495 
iteration = 45 | loss = 7.859338 
iteration = 50 | loss = 8.099228 
iteration = 55 | loss = 7.684679 
iteration = 60 | loss = 7.674637 
iteration = 65 | loss = 7.506339 
iteration = 70 | loss = 7.565728 
iteration = 75 | loss = 7.701934 
iteration = 80 | loss = 7.145021 
iteration = 85 | loss = 7.169829 
iteration = 90 | loss = 6.996316 
iteration = 95 | loss = 6.810837 
iteration = 100 | loss = 7.300989 
iteration = 105 | loss = 6.952348 
/scratch/xinshengwang/software/anaconda3/lib/python3.6/site-packages/torch/nn/functional.py:1340: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.
  warnings.warn("nn.functional.tanh is deprecated. Use torch.tanh instead.")
iteration = 0 | loss = 6.934171 
iteration = 5 | loss = 7.149777 
iteration = 10 | loss = 6.818755 
iteration = 15 | loss = 6.481771 
iteration = 20 | loss = 6.926781 
iteration = 25 | loss = 6.494823 
iteration = 30 | loss = 6.629725 
iteration = 35 | loss = 6.234308 
iteration = 40 | loss = 6.471769 
iteration = 45 | loss = 6.032393 
iteration = 50 | loss = 6.706580 
iteration = 55 | loss = 6.317075 
iteration = 60 | loss = 6.732183 
iteration = 65 | loss = 6.761419 
iteration = 70 | loss = 6.023965 
iteration = 75 | loss = 6.442039 
iteration = 80 | loss = 5.715672 
iteration = 85 | loss = 6.099917 
iteration = 90 | loss = 6.002199 
iteration = 95 | loss = 6.183076 
iteration = 100 | loss = 5.664534 
iteration = 105 | loss = 5.756223 
iteration = 0 | loss = 5.756191 
iteration = 5 | loss = 5.838156 
iteration = 10 | loss = 5.755210 
iteration = 15 | loss = 6.134120 
iteration = 20 | loss = 5.862026 
iteration = 25 | loss = 5.619078 
iteration = 30 | loss = 5.826190 
iteration = 35 | loss = 5.719524 
iteration = 40 | loss = 5.865103 
iteration = 45 | loss = 5.271226 
iteration = 50 | loss = 5.704281 
iteration = 55 | loss = 5.741519 
iteration = 60 | loss = 5.273975 
iteration = 65 | loss = 5.441688 
iteration = 70 | loss = 5.268826 
iteration = 75 | loss = 5.285912 
iteration = 80 | loss = 5.293497 
iteration = 85 | loss = 6.274978 
iteration = 90 | loss = 5.266185 
iteration = 95 | loss = 5.690640 
iteration = 100 | loss = 6.084438 
iteration = 105 | loss = 5.823268 
iteration = 0 | loss = 5.570143 
iteration = 5 | loss = 5.525804 
iteration = 10 | loss = 5.104826 
iteration = 15 | loss = 5.274558 
iteration = 20 | loss = 5.437085 
iteration = 25 | loss = 5.315516 
iteration = 30 | loss = 5.534361 
iteration = 35 | loss = 4.823556 
iteration = 40 | loss = 5.486452 
iteration = 45 | loss = 4.822305 
iteration = 50 | loss = 5.218682 
iteration = 55 | loss = 5.286143 
iteration = 60 | loss = 4.991995 
iteration = 65 | loss = 4.881082 
iteration = 70 | loss = 5.478065 
iteration = 75 | loss = 5.220220 
iteration = 80 | loss = 5.150056 
iteration = 85 | loss = 4.955235 
iteration = 90 | loss = 4.292422 
iteration = 95 | loss = 5.010440 
iteration = 100 | loss = 5.375176 
iteration = 105 | loss = 4.989374 
iteration = 0 | loss = 4.967572 
iteration = 5 | loss = 5.054966 
iteration = 10 | loss = 4.898134 
iteration = 15 | loss = 4.951099 
iteration = 20 | loss = 4.544116 
iteration = 25 | loss = 4.481263 
iteration = 30 | loss = 4.683639 
iteration = 35 | loss = 4.500916 
iteration = 40 | loss = 5.163228 
iteration = 45 | loss = 4.842287 
iteration = 50 | loss = 4.853743 
iteration = 55 | loss = 4.995190 
iteration = 60 | loss = 4.696726 
iteration = 65 | loss = 5.196006 
iteration = 70 | loss = 5.338817 
iteration = 75 | loss = 5.128901 
iteration = 80 | loss = 5.266541 
iteration = 85 | loss = 4.575038 
iteration = 90 | loss = 4.910569 
iteration = 95 | loss = 4.583121 
iteration = 100 | loss = 4.518514 
iteration = 105 | loss = 5.033874 
 Epoch: [5] Loss: 4.9070  R1_I2A: 0.4771 R1_A2I: 0.3331 
                 
iteration = 0 | loss = 4.607919 
iteration = 5 | loss = 4.331633 
iteration = 10 | loss = 5.179530 
iteration = 15 | loss = 4.349487 
iteration = 20 | loss = 4.494034 
iteration = 25 | loss = 5.006532 
iteration = 30 | loss = 4.781919 
iteration = 35 | loss = 4.815975 
iteration = 40 | loss = 4.899364 
iteration = 45 | loss = 4.458419 
iteration = 50 | loss = 4.869806 
iteration = 55 | loss = 5.564673 
iteration = 60 | loss = 4.509082 
iteration = 65 | loss = 4.832631 
iteration = 70 | loss = 4.932262 
iteration = 75 | loss = 4.668201 
iteration = 80 | loss = 4.665993 
iteration = 85 | loss = 4.511461 
iteration = 90 | loss = 4.716882 
iteration = 95 | loss = 4.563341 
iteration = 100 | loss = 5.310473 
iteration = 105 | loss = 4.835776 
iteration = 0 | loss = 4.315339 
iteration = 5 | loss = 4.287268 
iteration = 10 | loss = 4.524626 
iteration = 15 | loss = 4.271564 
iteration = 20 | loss = 4.639176 
iteration = 25 | loss = 4.662609 
iteration = 30 | loss = 4.133041 
iteration = 35 | loss = 4.350255 
iteration = 40 | loss = 5.113256 
iteration = 45 | loss = 4.504771 
iteration = 50 | loss = 4.850382 
iteration = 55 | loss = 4.074220 
iteration = 60 | loss = 4.381818 
iteration = 65 | loss = 4.756865 
iteration = 70 | loss = 4.822943 
iteration = 75 | loss = 4.522341 
iteration = 80 | loss = 4.417050 
iteration = 85 | loss = 4.884717 
iteration = 90 | loss = 4.838200 
iteration = 95 | loss = 4.870576 
iteration = 100 | loss = 4.486673 
iteration = 105 | loss = 5.003900 
iteration = 0 | loss = 4.197434 
iteration = 5 | loss = 4.503739 
iteration = 10 | loss = 4.781770 
iteration = 15 | loss = 4.172668 
iteration = 20 | loss = 4.085904 
iteration = 25 | loss = 4.626398 
iteration = 30 | loss = 4.470927 
iteration = 35 | loss = 4.658359 
iteration = 40 | loss = 4.332928 
iteration = 45 | loss = 4.655346 
iteration = 50 | loss = 4.600990 
iteration = 55 | loss = 4.831491 
iteration = 60 | loss = 4.106000 
iteration = 65 | loss = 4.124154 
iteration = 70 | loss = 4.473862 
iteration = 75 | loss = 4.635634 
iteration = 80 | loss = 4.436662 
iteration = 85 | loss = 4.247109 
iteration = 90 | loss = 4.314643 
iteration = 95 | loss = 4.428912 
iteration = 100 | loss = 4.498561 
iteration = 105 | loss = 4.249828 
iteration = 0 | loss = 4.193693 
iteration = 5 | loss = 4.111041 
iteration = 10 | loss = 3.988267 
iteration = 15 | loss = 4.326365 
iteration = 20 | loss = 4.366697 
iteration = 25 | loss = 5.028867 
iteration = 30 | loss = 4.165031 
iteration = 35 | loss = 4.464273 
iteration = 40 | loss = 3.839359 
iteration = 45 | loss = 3.974936 
iteration = 50 | loss = 4.443354 
iteration = 55 | loss = 4.309902 
iteration = 60 | loss = 4.737945 
iteration = 65 | loss = 4.581743 
iteration = 70 | loss = 5.025702 
iteration = 75 | loss = 3.904634 
iteration = 80 | loss = 4.558457 
iteration = 85 | loss = 4.591889 
iteration = 90 | loss = 4.427955 
iteration = 95 | loss = 4.236626 
iteration = 100 | loss = 4.260578 
iteration = 105 | loss = 3.755032 
iteration = 0 | loss = 3.600815 
iteration = 5 | loss = 4.695274 
iteration = 10 | loss = 4.572486 
iteration = 15 | loss = 3.886461 
iteration = 20 | loss = 4.096908 
iteration = 25 | loss = 3.750161 
iteration = 30 | loss = 4.504436 
iteration = 35 | loss = 4.209327 
iteration = 40 | loss = 4.393276 
iteration = 45 | loss = 4.770580 
iteration = 50 | loss = 4.117360 
iteration = 55 | loss = 4.529311 
iteration = 60 | loss = 4.175277 
iteration = 65 | loss = 4.500445 
iteration = 70 | loss = 4.633606 
iteration = 75 | loss = 3.648993 
iteration = 80 | loss = 4.336784 
iteration = 85 | loss = 4.460977 
iteration = 90 | loss = 4.581964 
iteration = 95 | loss = 4.462873 
iteration = 100 | loss = 4.371224 
iteration = 105 | loss = 3.956317 
 Epoch: [10] Loss: 4.2697  R1_I2A: 0.5316 R1_A2I: 0.4002 
                 
iteration = 0 | loss = 4.450648 
iteration = 5 | loss = 4.040065 
iteration = 10 | loss = 4.498749 
iteration = 15 | loss = 4.625465 
iteration = 20 | loss = 4.306965 
iteration = 25 | loss = 4.250120 
iteration = 30 | loss = 3.802642 
iteration = 35 | loss = 3.681823 
iteration = 40 | loss = 4.265061 
iteration = 45 | loss = 4.551971 
iteration = 50 | loss = 4.292597 
iteration = 55 | loss = 4.158647 
iteration = 60 | loss = 4.255078 
iteration = 65 | loss = 4.242547 
iteration = 70 | loss = 4.084458 
iteration = 75 | loss = 3.517352 
iteration = 80 | loss = 4.291289 
iteration = 85 | loss = 4.071700 
iteration = 90 | loss = 3.869682 
iteration = 95 | loss = 3.895579 
iteration = 100 | loss = 3.975547 
iteration = 105 | loss = 4.054712 
iteration = 0 | loss = 4.320041 
iteration = 5 | loss = 4.385604 
iteration = 10 | loss = 4.121339 
iteration = 15 | loss = 4.043835 
iteration = 20 | loss = 4.065598 
iteration = 25 | loss = 4.250360 
iteration = 30 | loss = 4.325583 
iteration = 35 | loss = 3.563128 
iteration = 40 | loss = 4.538556 
iteration = 45 | loss = 4.423805 
iteration = 50 | loss = 4.151559 
iteration = 55 | loss = 3.762805 
iteration = 60 | loss = 3.695539 
iteration = 65 | loss = 3.927310 
iteration = 70 | loss = 3.717636 
iteration = 75 | loss = 4.226252 
iteration = 80 | loss = 3.927351 
iteration = 85 | loss = 3.524000 
iteration = 90 | loss = 3.447130 
iteration = 95 | loss = 3.799623 
iteration = 100 | loss = 4.212726 
iteration = 105 | loss = 3.983260 
iteration = 0 | loss = 4.371081 
iteration = 5 | loss = 3.601853 
iteration = 10 | loss = 4.367894 
iteration = 15 | loss = 3.549273 
iteration = 20 | loss = 3.691538 
iteration = 25 | loss = 3.713016 
iteration = 30 | loss = 4.201992 
iteration = 35 | loss = 4.487851 
iteration = 40 | loss = 3.958854 
iteration = 45 | loss = 4.137885 
iteration = 50 | loss = 3.963683 
iteration = 55 | loss = 4.062986 
iteration = 60 | loss = 3.687808 
iteration = 65 | loss = 3.871557 
iteration = 70 | loss = 3.586113 
iteration = 75 | loss = 3.675541 
iteration = 80 | loss = 4.556258 
iteration = 85 | loss = 3.859495 
iteration = 90 | loss = 3.961139 
iteration = 95 | loss = 3.596036 
iteration = 100 | loss = 4.156900 
iteration = 105 | loss = 3.385702 
iteration = 0 | loss = 4.289793 
iteration = 5 | loss = 3.334993 
iteration = 10 | loss = 4.262157 
iteration = 15 | loss = 4.010072 
iteration = 20 | loss = 3.853616 
iteration = 25 | loss = 3.935084 
iteration = 30 | loss = 3.869090 
iteration = 35 | loss = 3.775108 
iteration = 40 | loss = 3.996390 
iteration = 45 | loss = 3.589852 
iteration = 50 | loss = 3.286976 
iteration = 55 | loss = 3.590641 
iteration = 60 | loss = 4.498381 
iteration = 65 | loss = 3.876956 
iteration = 70 | loss = 3.655413 
iteration = 75 | loss = 3.955585 
iteration = 80 | loss = 4.128653 
iteration = 85 | loss = 3.734098 
iteration = 90 | loss = 4.019847 
iteration = 95 | loss = 4.188626 
iteration = 100 | loss = 3.397120 
iteration = 105 | loss = 3.480400 
iteration = 0 | loss = 4.292773 
iteration = 5 | loss = 3.406358 
iteration = 10 | loss = 4.010275 
iteration = 15 | loss = 3.446133 
iteration = 20 | loss = 3.988716 
iteration = 25 | loss = 4.321671 
iteration = 30 | loss = 3.506475 
iteration = 35 | loss = 3.493009 
iteration = 40 | loss = 3.578323 
iteration = 45 | loss = 4.086753 
iteration = 50 | loss = 4.202478 
iteration = 55 | loss = 3.322948 
iteration = 60 | loss = 4.052498 
iteration = 65 | loss = 3.850827 
iteration = 70 | loss = 3.881174 
iteration = 75 | loss = 4.571180 
iteration = 80 | loss = 4.220565 
iteration = 85 | loss = 3.445525 
iteration = 90 | loss = 3.282380 
iteration = 95 | loss = 3.696167 
iteration = 100 | loss = 3.909310 
iteration = 105 | loss = 3.976215 
 Epoch: [15] Loss: 4.1990  R1_I2A: 0.5394 R1_A2I: 0.4249 
                 
iteration = 0 | loss = 4.440812 
iteration = 5 | loss = 4.067083 
iteration = 10 | loss = 3.704310 
iteration = 15 | loss = 3.808294 
iteration = 20 | loss = 3.423933 
iteration = 25 | loss = 3.815907 
iteration = 30 | loss = 3.723377 
iteration = 35 | loss = 4.370392 
iteration = 40 | loss = 3.579301 
iteration = 45 | loss = 4.134587 
iteration = 50 | loss = 3.904073 
iteration = 55 | loss = 3.600341 
iteration = 60 | loss = 3.948416 
iteration = 65 | loss = 4.268398 
iteration = 70 | loss = 3.720432 
iteration = 75 | loss = 3.961240 
iteration = 80 | loss = 3.361714 
iteration = 85 | loss = 3.796503 
iteration = 90 | loss = 3.251830 
iteration = 95 | loss = 3.731417 
iteration = 100 | loss = 3.795301 
iteration = 105 | loss = 3.499109 
iteration = 0 | loss = 3.360970 
iteration = 5 | loss = 4.402168 
iteration = 10 | loss = 3.198267 
iteration = 15 | loss = 3.582551 
iteration = 20 | loss = 3.716570 
iteration = 25 | loss = 3.684414 
iteration = 30 | loss = 4.010632 
iteration = 35 | loss = 3.820731 
iteration = 40 | loss = 3.929805 
iteration = 45 | loss = 3.387631 
iteration = 50 | loss = 3.320053 
iteration = 55 | loss = 3.551453 
iteration = 60 | loss = 4.049551 
iteration = 65 | loss = 3.215726 
iteration = 70 | loss = 3.620700 
iteration = 75 | loss = 3.948025 
iteration = 80 | loss = 3.288801 
iteration = 85 | loss = 4.092299 
iteration = 90 | loss = 3.609976 
iteration = 95 | loss = 3.658423 
iteration = 100 | loss = 3.946448 
iteration = 105 | loss = 3.588948 
iteration = 0 | loss = 3.082952 
iteration = 5 | loss = 3.100087 
iteration = 10 | loss = 3.200756 
iteration = 15 | loss = 3.795383 
iteration = 20 | loss = 3.328711 
iteration = 25 | loss = 3.891808 
iteration = 30 | loss = 3.632750 
iteration = 35 | loss = 4.385670 
iteration = 40 | loss = 3.848915 
iteration = 45 | loss = 3.314061 
iteration = 50 | loss = 4.019958 
iteration = 55 | loss = 4.020117 
iteration = 60 | loss = 3.818739 
iteration = 65 | loss = 3.302202 
iteration = 70 | loss = 3.299511 
iteration = 75 | loss = 3.607988 
iteration = 80 | loss = 3.829034 
iteration = 85 | loss = 3.386322 
iteration = 90 | loss = 3.776081 
iteration = 95 | loss = 3.360050 
iteration = 100 | loss = 3.596472 
iteration = 105 | loss = 3.184858 
iteration = 0 | loss = 3.712353 
iteration = 5 | loss = 3.368767 
iteration = 10 | loss = 3.364541 
iteration = 15 | loss = 3.954784 
iteration = 20 | loss = 3.597057 
iteration = 25 | loss = 3.599208 
iteration = 30 | loss = 3.392014 
iteration = 35 | loss = 4.087594 
iteration = 40 | loss = 4.256799 
iteration = 45 | loss = 3.214881 
iteration = 50 | loss = 3.133531 
iteration = 55 | loss = 3.571173 
iteration = 60 | loss = 3.528812 
iteration = 65 | loss = 3.074831 
iteration = 70 | loss = 2.957649 
iteration = 75 | loss = 3.834280 
iteration = 80 | loss = 3.788816 
iteration = 85 | loss = 3.392901 
iteration = 90 | loss = 3.831279 
iteration = 95 | loss = 3.503354 
iteration = 100 | loss = 3.624077 
iteration = 105 | loss = 3.500533 
iteration = 0 | loss = 3.500082 
iteration = 5 | loss = 3.691038 
iteration = 10 | loss = 3.551373 
iteration = 15 | loss = 3.668802 
iteration = 20 | loss = 3.646188 
iteration = 25 | loss = 3.545810 
iteration = 30 | loss = 3.213482 
iteration = 35 | loss = 3.026590 
iteration = 40 | loss = 3.093319 
iteration = 45 | loss = 3.933093 
iteration = 50 | loss = 3.542470 
iteration = 55 | loss = 3.237690 
iteration = 60 | loss = 3.239160 
iteration = 65 | loss = 3.450991 
iteration = 70 | loss = 3.804281 
iteration = 75 | loss = 3.592184 
iteration = 80 | loss = 3.538432 
iteration = 85 | loss = 3.296670 
iteration = 90 | loss = 3.464630 
iteration = 95 | loss = 3.678221 
iteration = 100 | loss = 3.413267 
iteration = 105 | loss = 3.464271 
 Epoch: [20] Loss: 3.5521  R1_I2A: 0.5714 R1_A2I: 0.4347 
                 
iteration = 0 | loss = 3.616115 
iteration = 5 | loss = 3.938619 
iteration = 10 | loss = 3.569232 
iteration = 15 | loss = 3.965209 
iteration = 20 | loss = 3.415500 
iteration = 25 | loss = 3.671911 
iteration = 30 | loss = 3.595568 
iteration = 35 | loss = 3.232211 
iteration = 40 | loss = 3.453016 
iteration = 45 | loss = 3.999101 
iteration = 50 | loss = 3.473042 
iteration = 55 | loss = 3.483922 
iteration = 60 | loss = 3.177503 
iteration = 65 | loss = 4.026117 
iteration = 70 | loss = 3.330291 
iteration = 75 | loss = 3.782218 
iteration = 80 | loss = 3.283171 
iteration = 85 | loss = 2.996242 
iteration = 90 | loss = 3.563716 
iteration = 95 | loss = 3.502882 
iteration = 100 | loss = 3.024775 
iteration = 105 | loss = 3.628356 
iteration = 0 | loss = 3.258986 
iteration = 5 | loss = 3.746865 
iteration = 10 | loss = 3.262135 
iteration = 15 | loss = 3.618127 
iteration = 20 | loss = 3.374937 
iteration = 25 | loss = 3.482742 
iteration = 30 | loss = 3.410326 
iteration = 35 | loss = 3.020118 
iteration = 40 | loss = 3.112391 
iteration = 45 | loss = 3.289464 
iteration = 50 | loss = 3.709440 
iteration = 55 | loss = 3.428770 
iteration = 60 | loss = 3.259364 
iteration = 65 | loss = 3.666757 
iteration = 70 | loss = 3.828799 
iteration = 75 | loss = 3.338135 
iteration = 80 | loss = 3.596424 
iteration = 85 | loss = 3.227232 
iteration = 90 | loss = 2.992215 
iteration = 95 | loss = 3.120603 
iteration = 100 | loss = 3.770285 
iteration = 105 | loss = 3.307732 
iteration = 0 | loss = 3.486651 
iteration = 5 | loss = 3.081172 
iteration = 10 | loss = 3.625708 
iteration = 15 | loss = 3.310228 
iteration = 20 | loss = 3.349207 
iteration = 25 | loss = 2.624217 
iteration = 30 | loss = 3.179990 
iteration = 35 | loss = 3.685921 
iteration = 40 | loss = 3.465990 
iteration = 45 | loss = 3.310321 
iteration = 50 | loss = 3.326088 
iteration = 55 | loss = 3.524968 
iteration = 60 | loss = 3.376660 
iteration = 65 | loss = 3.390986 
iteration = 70 | loss = 3.496372 
iteration = 75 | loss = 3.376688 
iteration = 80 | loss = 3.084874 
iteration = 85 | loss = 3.236263 
iteration = 90 | loss = 3.422123 
iteration = 95 | loss = 3.034731 
iteration = 100 | loss = 3.963182 
iteration = 105 | loss = 2.966155 
iteration = 0 | loss = 3.390766 
iteration = 5 | loss = 3.411697 
iteration = 10 | loss = 3.404743 
iteration = 15 | loss = 2.836504 
iteration = 20 | loss = 3.393003 
iteration = 25 | loss = 3.606014 
iteration = 30 | loss = 3.491981 
iteration = 35 | loss = 3.370822 
iteration = 40 | loss = 3.030958 
iteration = 45 | loss = 3.292132 
iteration = 50 | loss = 3.464956 
iteration = 55 | loss = 3.056656 
iteration = 60 | loss = 2.884082 
iteration = 65 | loss = 3.181366 
iteration = 70 | loss = 3.445613 
iteration = 75 | loss = 3.426193 
iteration = 80 | loss = 3.638601 
iteration = 85 | loss = 3.713399 
iteration = 90 | loss = 3.765115 
iteration = 95 | loss = 2.929399 
iteration = 100 | loss = 3.546119 
iteration = 105 | loss = 3.147296 
iteration = 0 | loss = 3.545852 
iteration = 5 | loss = 3.539982 
iteration = 10 | loss = 3.232448 
iteration = 15 | loss = 3.077618 
iteration = 20 | loss = 3.348152 
iteration = 25 | loss = 3.304182 
iteration = 30 | loss = 3.446585 
iteration = 35 | loss = 3.578028 
iteration = 40 | loss = 3.339920 
iteration = 45 | loss = 3.520416 
iteration = 50 | loss = 3.118304 
iteration = 55 | loss = 3.905935 
iteration = 60 | loss = 3.360661 
iteration = 65 | loss = 3.323684 
iteration = 70 | loss = 3.169475 
iteration = 75 | loss = 3.389388 
iteration = 80 | loss = 3.496014 
iteration = 85 | loss = 3.548683 
iteration = 90 | loss = 3.059831 
iteration = 95 | loss = 3.737096 
iteration = 100 | loss = 3.339790 
iteration = 105 | loss = 3.126057 
 Epoch: [25] Loss: 3.5206  R1_I2A: 0.5541 R1_A2I: 0.4533 
                 
 Epoch: [25] Loss: 3.5206  R1_I2A: 0.5581 mAP_I2A: 0.5005  R1_A2I: 0.4533 mAP_A2I: 0.4002 
                     
iteration = 0 | loss = 2.690035 
iteration = 5 | loss = 3.550274 
iteration = 10 | loss = 3.508512 
iteration = 15 | loss = 3.689568 
iteration = 20 | loss = 3.008521 
iteration = 25 | loss = 3.153764 
iteration = 30 | loss = 3.648980 
iteration = 35 | loss = 2.714933 
iteration = 40 | loss = 2.873643 
iteration = 45 | loss = 3.126123 
iteration = 50 | loss = 3.751474 
iteration = 55 | loss = 3.082084 
iteration = 60 | loss = 3.229644 
iteration = 65 | loss = 3.364622 
iteration = 70 | loss = 3.592085 
iteration = 75 | loss = 3.160579 
iteration = 80 | loss = 3.795946 
iteration = 85 | loss = 3.036446 
iteration = 90 | loss = 3.801114 
iteration = 95 | loss = 3.323062 
iteration = 100 | loss = 3.919155 
iteration = 105 | loss = 2.890121 
iteration = 0 | loss = 3.165086 
iteration = 5 | loss = 3.375323 
iteration = 10 | loss = 3.094534 
iteration = 15 | loss = 2.994244 
iteration = 20 | loss = 2.730198 
iteration = 25 | loss = 3.387388 
iteration = 30 | loss = 3.720846 
iteration = 35 | loss = 3.290471 
iteration = 40 | loss = 3.379458 
iteration = 45 | loss = 3.199049 
iteration = 50 | loss = 3.095862 
iteration = 55 | loss = 3.736842 
iteration = 60 | loss = 3.639192 
iteration = 65 | loss = 3.375432 
iteration = 70 | loss = 3.081393 
iteration = 75 | loss = 3.381114 
iteration = 80 | loss = 3.284662 
iteration = 85 | loss = 3.334112 
iteration = 90 | loss = 2.938017 
iteration = 95 | loss = 2.843885 
iteration = 100 | loss = 4.011999 
iteration = 105 | loss = 3.400671 
iteration = 0 | loss = 3.446987 
iteration = 5 | loss = 3.123835 
iteration = 10 | loss = 3.227043 
iteration = 15 | loss = 3.248311 
iteration = 20 | loss = 3.859282 
iteration = 25 | loss = 2.844534 
iteration = 30 | loss = 3.494127 
iteration = 35 | loss = 2.961380 
iteration = 40 | loss = 3.141051 
iteration = 45 | loss = 2.816358 
iteration = 50 | loss = 3.313673 
iteration = 55 | loss = 3.040002 
iteration = 60 | loss = 3.193102 
iteration = 65 | loss = 3.247772 
iteration = 70 | loss = 2.879016 
iteration = 75 | loss = 3.209133 
iteration = 80 | loss = 3.122330 
iteration = 85 | loss = 3.306405 
iteration = 90 | loss = 3.785979 
iteration = 95 | loss = 2.652390 
iteration = 100 | loss = 3.168007 
iteration = 105 | loss = 3.637162 
iteration = 0 | loss = 3.483347 
iteration = 5 | loss = 3.424772 
iteration = 10 | loss = 2.995908 
iteration = 15 | loss = 3.931746 
iteration = 20 | loss = 3.045174 
iteration = 25 | loss = 3.507146 
iteration = 30 | loss = 3.150352 
iteration = 35 | loss = 3.521755 
iteration = 40 | loss = 3.345929 
iteration = 45 | loss = 3.001326 
iteration = 50 | loss = 3.343338 
iteration = 55 | loss = 3.091653 
iteration = 60 | loss = 3.230367 
iteration = 65 | loss = 3.911189 
iteration = 70 | loss = 3.184446 
iteration = 75 | loss = 3.583938 
iteration = 80 | loss = 2.890978 
iteration = 85 | loss = 3.105314 
iteration = 90 | loss = 3.368285 
iteration = 95 | loss = 3.145324 
iteration = 100 | loss = 2.942674 
iteration = 105 | loss = 3.566368 
iteration = 0 | loss = 2.894319 
iteration = 5 | loss = 2.982098 
iteration = 10 | loss = 3.174129 
iteration = 15 | loss = 3.124726 
iteration = 20 | loss = 2.965345 
iteration = 25 | loss = 3.183122 
iteration = 30 | loss = 2.331774 
iteration = 35 | loss = 3.044991 
iteration = 40 | loss = 3.668700 
iteration = 45 | loss = 3.183944 
iteration = 50 | loss = 3.394933 
iteration = 55 | loss = 4.190744 
iteration = 60 | loss = 3.022185 
iteration = 65 | loss = 3.376710 
iteration = 70 | loss = 3.846071 
iteration = 75 | loss = 3.422166 
iteration = 80 | loss = 3.111755 
iteration = 85 | loss = 2.784858 
iteration = 90 | loss = 3.328201 
iteration = 95 | loss = 3.156850 
iteration = 100 | loss = 3.116997 
iteration = 105 | loss = 3.341023 
 Epoch: [30] Loss: 2.9788  R1_I2A: 0.5506 R1_A2I: 0.4583 
                 
 Epoch: [30] Loss: 2.9788  R1_I2A: 0.5497 mAP_I2A: 0.4973  R1_A2I: 0.4583 mAP_A2I: 0.4006 
                     
iteration = 0 | loss = 3.197625 
iteration = 5 | loss = 3.491374 
iteration = 10 | loss = 2.961910 
iteration = 15 | loss = 3.450689 
iteration = 20 | loss = 3.287523 
iteration = 25 | loss = 3.267550 
iteration = 30 | loss = 2.719566 
iteration = 35 | loss = 3.182772 
iteration = 40 | loss = 2.813920 
iteration = 45 | loss = 2.993876 
iteration = 50 | loss = 3.277966 
iteration = 55 | loss = 2.712822 
iteration = 60 | loss = 3.209284 
iteration = 65 | loss = 3.483113 
iteration = 70 | loss = 2.807455 
iteration = 75 | loss = 3.177401 
iteration = 80 | loss = 3.306031 
iteration = 85 | loss = 2.973731 
iteration = 90 | loss = 3.103218 
iteration = 95 | loss = 3.329436 
iteration = 100 | loss = 3.818408 
iteration = 105 | loss = 3.190458 
iteration = 0 | loss = 3.492330 
iteration = 5 | loss = 2.823696 
iteration = 10 | loss = 3.429190 
iteration = 15 | loss = 2.933444 
iteration = 20 | loss = 3.324059 
iteration = 25 | loss = 2.990904 
iteration = 30 | loss = 3.145327 
iteration = 35 | loss = 3.428706 
iteration = 40 | loss = 3.418497 
iteration = 45 | loss = 3.472949 
iteration = 50 | loss = 3.025340 
iteration = 55 | loss = 3.480554 
iteration = 60 | loss = 2.997704 
iteration = 65 | loss = 3.040067 
iteration = 70 | loss = 2.822536 
iteration = 75 | loss = 3.481257 
iteration = 80 | loss = 3.410563 
iteration = 85 | loss = 2.879095 
iteration = 90 | loss = 3.020936 
iteration = 95 | loss = 3.387218 
iteration = 100 | loss = 2.678196 
iteration = 105 | loss = 2.994037 
iteration = 0 | loss = 3.031126 
iteration = 5 | loss = 2.875373 
iteration = 10 | loss = 3.439642 
iteration = 15 | loss = 3.655704 
iteration = 20 | loss = 3.478185 
iteration = 25 | loss = 3.006895 
iteration = 30 | loss = 2.688720 
iteration = 35 | loss = 3.615036 
iteration = 40 | loss = 2.895453 
iteration = 45 | loss = 2.740599 
iteration = 50 | loss = 3.711712 
iteration = 55 | loss = 3.215916 
iteration = 60 | loss = 3.136723 
iteration = 65 | loss = 2.871874 
iteration = 70 | loss = 2.896362 
iteration = 75 | loss = 3.020538 
iteration = 80 | loss = 3.016340 
iteration = 85 | loss = 3.410766 
iteration = 90 | loss = 3.049447 
iteration = 95 | loss = 3.149217 
iteration = 100 | loss = 3.725254 
iteration = 105 | loss = 2.963317 
iteration = 0 | loss = 3.296534 
iteration = 5 | loss = 3.105522 
iteration = 10 | loss = 2.931769 
iteration = 15 | loss = 3.393548 
iteration = 20 | loss = 2.872076 
iteration = 25 | loss = 3.067725 
iteration = 30 | loss = 3.084795 
iteration = 35 | loss = 3.195506 
iteration = 40 | loss = 2.639471 
iteration = 45 | loss = 3.266359 
iteration = 50 | loss = 2.844140 
iteration = 55 | loss = 3.112637 
iteration = 60 | loss = 2.926335 
iteration = 65 | loss = 3.229501 
iteration = 70 | loss = 2.929506 
iteration = 75 | loss = 2.503559 
iteration = 80 | loss = 3.166290 
iteration = 85 | loss = 2.901845 
iteration = 90 | loss = 2.909814 
iteration = 95 | loss = 3.351128 
iteration = 100 | loss = 3.846364 
iteration = 105 | loss = 3.423054 
iteration = 0 | loss = 2.997850 
iteration = 5 | loss = 3.417542 
iteration = 10 | loss = 2.900899 
iteration = 15 | loss = 2.935872 
iteration = 20 | loss = 3.241608 
iteration = 25 | loss = 3.042990 
iteration = 30 | loss = 3.274716 
iteration = 35 | loss = 3.176509 
iteration = 40 | loss = 2.566137 
iteration = 45 | loss = 3.088806 
iteration = 50 | loss = 2.958034 
iteration = 55 | loss = 2.745003 
iteration = 60 | loss = 2.543293 
iteration = 65 | loss = 3.380142 
iteration = 70 | loss = 2.861549 
iteration = 75 | loss = 2.995897 
iteration = 80 | loss = 2.868673 
iteration = 85 | loss = 3.028908 
iteration = 90 | loss = 2.813217 
iteration = 95 | loss = 3.845336 
iteration = 100 | loss = 3.077628 
iteration = 105 | loss = 3.245519 
 Epoch: [35] Loss: 3.3326  R1_I2A: 0.5636 R1_A2I: 0.4790 
                 
 Epoch: [35] Loss: 3.3326  R1_I2A: 0.5658 mAP_I2A: 0.5086  R1_A2I: 0.4790 mAP_A2I: 0.4094 
                     
iteration = 0 | loss = 2.801910 
iteration = 5 | loss = 2.503633 
iteration = 10 | loss = 3.354920 
iteration = 15 | loss = 2.984961 
iteration = 20 | loss = 2.486842 
iteration = 25 | loss = 2.567208 
iteration = 30 | loss = 2.596663 
iteration = 35 | loss = 2.673268 
iteration = 40 | loss = 3.547616 
iteration = 45 | loss = 3.296576 
iteration = 50 | loss = 2.993626 
iteration = 55 | loss = 2.965438 
iteration = 60 | loss = 2.881305 
iteration = 65 | loss = 3.263297 
iteration = 70 | loss = 3.057700 
iteration = 75 | loss = 3.136316 
iteration = 80 | loss = 2.785873 
iteration = 85 | loss = 2.962701 
iteration = 90 | loss = 3.141242 
iteration = 95 | loss = 3.313170 
iteration = 100 | loss = 2.986185 
iteration = 105 | loss = 2.843472 
iteration = 0 | loss = 2.558009 
iteration = 5 | loss = 3.033290 
iteration = 10 | loss = 2.797400 
iteration = 15 | loss = 2.827822 
iteration = 20 | loss = 2.328410 
iteration = 25 | loss = 3.572559 
iteration = 30 | loss = 3.092422 
iteration = 35 | loss = 3.232251 
iteration = 40 | loss = 3.404488 
iteration = 45 | loss = 2.886222 
iteration = 50 | loss = 2.883591 
iteration = 55 | loss = 3.041533 
iteration = 60 | loss = 2.698274 
iteration = 65 | loss = 3.071159 
iteration = 70 | loss = 3.369511 
iteration = 75 | loss = 3.039563 
iteration = 80 | loss = 2.509917 
iteration = 85 | loss = 2.809549 
iteration = 90 | loss = 2.618412 
iteration = 95 | loss = 2.978189 
iteration = 100 | loss = 2.926909 
iteration = 105 | loss = 3.077220 
iteration = 0 | loss = 2.635851 
iteration = 5 | loss = 2.937644 
iteration = 10 | loss = 2.696063 
iteration = 15 | loss = 2.992277 
iteration = 20 | loss = 2.727445 
iteration = 25 | loss = 2.474353 
iteration = 30 | loss = 3.143140 
iteration = 35 | loss = 2.578360 
iteration = 40 | loss = 3.045566 
iteration = 45 | loss = 2.480193 
iteration = 50 | loss = 2.974786 
iteration = 55 | loss = 2.759064 
iteration = 60 | loss = 2.738833 
iteration = 65 | loss = 3.083993 
iteration = 70 | loss = 2.885239 
iteration = 75 | loss = 3.426205 
iteration = 80 | loss = 2.930345 
iteration = 85 | loss = 2.762937 
iteration = 90 | loss = 2.633652 
iteration = 95 | loss = 3.156387 
iteration = 100 | loss = 2.875616 
iteration = 105 | loss = 2.838169 
iteration = 0 | loss = 2.543505 
iteration = 5 | loss = 3.483967 
iteration = 10 | loss = 3.178230 
iteration = 15 | loss = 2.690165 
iteration = 20 | loss = 2.929100 
iteration = 25 | loss = 2.855831 
iteration = 30 | loss = 2.969092 
iteration = 35 | loss = 2.624466 
iteration = 40 | loss = 2.359368 
iteration = 45 | loss = 2.736539 
iteration = 50 | loss = 2.817615 
iteration = 55 | loss = 3.251137 
iteration = 60 | loss = 2.659224 
iteration = 65 | loss = 3.249598 
iteration = 70 | loss = 2.732139 
iteration = 75 | loss = 3.206087 
iteration = 80 | loss = 2.855824 
iteration = 85 | loss = 2.913437 
iteration = 90 | loss = 3.153522 
iteration = 95 | loss = 2.964600 
iteration = 100 | loss = 2.896634 
iteration = 105 | loss = 2.444559 
iteration = 0 | loss = 3.012100 
iteration = 5 | loss = 2.872427 
iteration = 10 | loss = 2.763463 
iteration = 15 | loss = 3.075362 
iteration = 20 | loss = 2.839222 
iteration = 25 | loss = 3.173964 
iteration = 30 | loss = 2.720503 
iteration = 35 | loss = 2.990123 
iteration = 40 | loss = 3.036568 
iteration = 45 | loss = 2.455323 
iteration = 50 | loss = 2.829483 
iteration = 55 | loss = 2.962209 
iteration = 60 | loss = 3.297767 
iteration = 65 | loss = 2.589084 
iteration = 70 | loss = 2.693012 
iteration = 75 | loss = 2.836796 
iteration = 80 | loss = 2.628781 
iteration = 85 | loss = 3.283402 
iteration = 90 | loss = 3.045600 
iteration = 95 | loss = 2.360928 
iteration = 100 | loss = 2.654267 
iteration = 105 | loss = 3.067501 
 Epoch: [40] Loss: 2.9269  R1_I2A: 0.6026 R1_A2I: 0.4698 
                 
 Epoch: [40] Loss: 2.9269  R1_I2A: 0.6051 mAP_I2A: 0.5298  R1_A2I: 0.4698 mAP_A2I: 0.4116 
                     
iteration = 0 | loss = 2.834033 
iteration = 5 | loss = 2.897063 
iteration = 10 | loss = 2.819937 
iteration = 15 | loss = 2.404404 
iteration = 20 | loss = 2.557236 
iteration = 25 | loss = 3.163989 
iteration = 30 | loss = 3.316487 
iteration = 35 | loss = 2.985423 
iteration = 40 | loss = 2.831760 
iteration = 45 | loss = 3.278013 
iteration = 50 | loss = 3.101035 
iteration = 55 | loss = 3.190282 
iteration = 60 | loss = 2.906425 
iteration = 65 | loss = 3.476970 
iteration = 70 | loss = 3.135699 
iteration = 75 | loss = 3.230791 
iteration = 80 | loss = 2.832773 
iteration = 85 | loss = 2.610491 
iteration = 90 | loss = 2.334266 
iteration = 95 | loss = 2.728426 
iteration = 100 | loss = 2.889182 
iteration = 105 | loss = 2.722766 
